{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"UAntwerpen-modules documentation overview Design decisions The CalcUA-init module The generic calcua and clusterarch modules Environment variables LMOD defaults and aliases Scripts The SitePackage.lua file and its included LUA files EasyBuild configuration Directory structure of the repositories and software stack Procedures Development Test scripts Design considerations that led to the design decisions.","title":"Overview"},{"location":"#uantwerpen-modules-documentation-overview","text":"Design decisions The CalcUA-init module The generic calcua and clusterarch modules Environment variables LMOD defaults and aliases Scripts The SitePackage.lua file and its included LUA files EasyBuild configuration Directory structure of the repositories and software stack Procedures Development Test scripts Design considerations that led to the design decisions.","title":"UAntwerpen-modules documentation overview"},{"location":"LMOD_defaults_aliases/","text":"LMOD defaults and aliases Aliases for the clusterarch modules There are two ways to define more user-friendly names for the clusterarch arch modules: Use module_alias statements in a modulerc file, e.g., module_alias ( 'cluster/vaughan' , 'arch/redhat8-zen2-noaccel' ) These will then show in a separate section of the module avail overview. Use module_version statements in a modulerc file, e.g., module_version ( 'arch/redhat8-zen2-noaccel' , 'vaughan' ) These will be shown as attributes for the corresponding arch module in the module avail overview. We want these alternative names to be separately defined for each software stack. The reasoning behind this is that for a special section of the cluster, e.g., a section with GPU accelerators, we may not have a separate architecture in one version of the software stack while we may have in the other, so we want the synonym to refer to different arch modules dependent on the software stack. There are again several ways to realise this that fit into the directory structure We can define an additional mgmt/LMOD subdirectory and store a separate modulerc file for each software stack in that directory. That file is then added to the LMOD_MODULERCFILE PATH-style variable with the list of modulerc files that should be used. The definitions can be put in a .modulercz file in the modules-infrastructure/arch/calcua/yyyyx/arch subdirectory. We have chosen for the second option . The modulerc file is generated by the prepare_ClusterMod.sh script based on information contained in the ClusterMod_ClusterMap variable of the system definition file, and is also re-generated during a repair run, so the file should not be hand-edited.","title":"LMOD defaults and aliases"},{"location":"LMOD_defaults_aliases/#lmod-defaults-and-aliases","text":"","title":"LMOD defaults and aliases"},{"location":"LMOD_defaults_aliases/#aliases-for-the-clusterarch-modules","text":"There are two ways to define more user-friendly names for the clusterarch arch modules: Use module_alias statements in a modulerc file, e.g., module_alias ( 'cluster/vaughan' , 'arch/redhat8-zen2-noaccel' ) These will then show in a separate section of the module avail overview. Use module_version statements in a modulerc file, e.g., module_version ( 'arch/redhat8-zen2-noaccel' , 'vaughan' ) These will be shown as attributes for the corresponding arch module in the module avail overview. We want these alternative names to be separately defined for each software stack. The reasoning behind this is that for a special section of the cluster, e.g., a section with GPU accelerators, we may not have a separate architecture in one version of the software stack while we may have in the other, so we want the synonym to refer to different arch modules dependent on the software stack. There are again several ways to realise this that fit into the directory structure We can define an additional mgmt/LMOD subdirectory and store a separate modulerc file for each software stack in that directory. That file is then added to the LMOD_MODULERCFILE PATH-style variable with the list of modulerc files that should be used. The definitions can be put in a .modulercz file in the modules-infrastructure/arch/calcua/yyyyx/arch subdirectory. We have chosen for the second option . The modulerc file is generated by the prepare_ClusterMod.sh script based on information contained in the ClusterMod_ClusterMap variable of the system definition file, and is also re-generated during a repair run, so the file should not be hand-edited.","title":"Aliases for the clusterarch modules"},{"location":"SitePackage/","text":"The SitePackage.lua file and its included routines SitePackage_map_toolchain.lua : Mapping regular toolchain names onto yyy.mm for comparisons to find matching versions of some files. SitePackage_arch_hierarchy.lua : Routines to work with the hierarchy of architectures and map between long and short names. SitePackage_system_info.lua : Routines to gather information about the system. SitePackage_helper.lua : Other routines and data structures. Some of the data structures, in particular those that need to be extended with each new toolchain and really define the setup of the system, are not in the LMOD subdirectory but instead in the file etc/SystemDefinition.lua . Naming conventions Full 3-component architecture: osarch (where possible) Full 2-component architecture without OS: arch (where possible) Supported OS as name + version: os 2-component architecture (CPU + accelerator): modarch Note that we discussed a 2-level scheme where we always use the shortest name possible (hence avoiding -noaccel ) but this is not implemented as we feel it may create confusion and as it also complicates the implementation. etc/SystemDefinition.lua TODO: Some clean-up. Some of these tables could be generated automatically in SitePackage_helper.lua ? ClusterMod_NodeTypes ClusterMod_NodeTypes is simply n array of nodes in the system, specified using the long os-CPU-accelerator names. As this is a description of the current hardware in the cluster, it is not for a specific version of the software stack. The table is used to produce output for debug purposes of this configuration file, e.g., to list which software stacks for which architectures will be available on which node types. Note that one should distinguish between the generic processor types and the real processor types. No node type in the table below should have a generic processor type in its name as that would cause problems with the 3-level software architecture schemes (and make coding more difficult even for the 2-level software architecture scheme). Example: At the time of writing, the CalcUA cluster description would have been: ClusterMod_NodeTypes = { 'redhat7-ivybridge-noaccel' , 'redhat7-broadwell-noaccel' , 'redhat8-broadwell-noaccel' , 'redhat8-broadwell-pascal' , 'redhat8-broadwell-P5000' , 'redhat8-skylake-noaccel' , 'redhat8-skylake-aurora1' , 'redhat8-zen2-noaccel' , 'redhat8-zen2-ampere' , 'redhat8-zen2-arcturus' , } ClusterMod_SystemTable ClusterMod_SystemTable defines the whole structure of the software tree, including the manually installed software and system-wide EasyBuild managed software. Note that the table will be completed automatically with more generic os-cpu-accelerator architecture strings based on the other tables in this file. All names used should be for the 3L scheme. However, the middle level should not be used for versions that will use a 2L_long naming scheme. ClusterMod_SystemTable = { [ 'system' ] = { [ 'redhat7' ] = { 'x86_64' , }, [ 'redhat8' ] = { 'x86_64' , }, }, [ 'manual' ] = { [ 'redhat7' ] = { 'x86_64' , }, [ 'redhat8' ] = { 'x86_64' , }, }, [ '2020a' ] = { [ 'redhat7' ] = { 'ivybridge-noaccel' , 'broadwell-noaccel' , }, [ 'redhat8' ] = { 'zen2-noaccel' , 'skylake-noaccel' , } }, [ '2021b' ] = { [ 'redhat7' ] = { 'ivybridge-noaccel' , }, [ 'redhat8' ] = { 'broadwell-noaccel' , 'zen2-noaccel' , 'skylake-noaccel' , } }, } The structure is a table of tables. The keys on the first level are the versions of the software stacks, with two special version: system and manual . The keys on the second level are the long OS versions (name+version as used in the architecture identifiers) The values are then the CPU architecture + accelerator for each software stack + OS combo. Only the most specific architecture needs to be given, the others are derived automatically from other tables. ClusterMod_SystemProperies ClusterMod_SystemProperies adds additional information for each partition that is not contained in ClusterMod_SystemTable . It is a table of tables, the first index is the name of the calcua stack or manual for the pseudo-stack of manually installed software. The table for each software stack has the following entries: EasyBuild : Default version of EasyBuild for this stack. Should not be defined for the manual stack. hierarchy : Type of architecture hierarchy used for this software stack. Currently only the first option is implemented: 2L : two levels, the least generic level always includes an accelerator field 3L : 3 levels in the architecture hierarchy. ClusterMod_ClusterMap ClusterMod_ClusterMap contains for each version of the calcua toolchains, including the dummy system version, a mapping from cluster names to os-architecture strings. Each should be the topmost supported architecture for a particular node type. ClusterMod_ClusterMap = { [ 'system' ] = { [ 'hopper' ] = 'redhat7-x86_64' , [ 'leibniz' ] = 'redhat8-x86_64' , [ 'leibniz-skl' ] = 'redhat8-x86_64' , [ 'vaughan' ] = 'redhat8-x86_64' , }, [ '2020a' ] = { [ 'hopper' ] = 'redhat7-ivybridge-noaccel' , [ 'leibniz' ] = 'redhat7-broadwell-noaccel' , [ 'leibniz-skl' ] = 'redhat8-skylake-noaccel' , [ 'vaughan' ] = 'redhat8-zen2-noaccel' , }, [ '2021b' ] = { [ 'hopper' ] = 'redhat7-ivybridge-noaccel' , [ 'leibniz' ] = 'redhat8-broadwell-noaccel' , [ 'leibniz-skl' ] = 'redhat8-skylake-noaccel' , [ 'vaughan' ] = 'redhat8-zen2-noaccel' , }, } ClusterMod_ClusterMap is an associative table-of-tables. On the first level, the keys are the various versions of the software stack visible to users (and can include system ) On the second level, the keys are the names of the clusters that we want to map on a particular architecture. The values are then the architecture strings, in the formats specified in ClusterMod_SystemProperties . ClusterMod_toolchain_map ClusterMod_toolchain_map is an associative table, with the yyyy[a|b] toolchains as the keys and the matching yyyymm value as the value (note: no dot, not yyyy.mm) Example: ClusterMod_toolchain_map = { [ 'system' ] = '200000' , [ 'manual' ] = '200000' , [ '2020a' ] = '202001' , [ '2020b' ] = '202007' , [ '2021a' ] = '202101' , [ '2021b' ] = '202107' , [ '2022a' ] = '202201' , } ClusterMod_map_arch_hierarchy ClusterMod_map_arch_hierarchy is an associative table of associative tables with for each supported OS a table that can be used to determine the parent of every CPU/accelerator architecture. First level: The keys are the yyyymm versions of toolchains, the values the matching associative table. These toolchain versions are \"starting from\", so not every toolchain needs to be specified. Second level: Associative table with as keys the CPU/GPU architecture string and as value the parent GPU/architecture string, or nil if it is the top (= most generic) architecture. Example: ClusterMod_map_arch_hierarchy = { -- We start with a 2-level map [ '200000' ] = { [ 'zen2-noaccel' ] = 'x86_64' , [ 'zen2-ampere' ] = 'x86_64' , [ 'zen2-arcturus' ] = 'x86_64' , [ 'broadwell-noaccel' ] = 'x86_64' , [ 'broadwell-P5000' ] = 'x86_64' , [ 'broadwell-pascal' ] = 'x86_64' , [ 'skylake-noaccel' ] = 'x86_64' , [ 'skylake-aurora1' ] = 'x86_64' , [ 'ivybridge-noaccel' ] = 'x86_64' , [ 'x86_64' ] = nil , } } TODO: Can we get rid of the above data structure? ClusterMod_def_cpu ClusterMod_def_cpu is an associative table of associative tables defining the CPU architectures and whether they are generic or not. First level: The keys are the yyyymm versions of toolchains, the values the matching associative table. These toolchain versions are \"starting from\", so not every toolchain needs to be specified. Second level: Associative table with as keys the CPU type (long names) and as value true for generic CPU architectures and false otherwise. The map is versioned, but do expect problems with finding the right version of the system stack for a regular stack if all of a sudden a regular CPU would become generic or vice-versa, so in practice it is very likely only one version will ever be needed as it can be safely extended with new types. Example: ClusterMod_def_cpu = { [ 'zen4' ] = false , [ 'zen3' ] = false , [ 'zen2' ] = false , [ 'skylake' ] = false , [ 'broadwell' ] = false , [ 'ivybridge' ] = false , [ 'x86_64' ] = true , } ClusterMod_map_cpu_to_gen ClusterMod_map_cpu_to_gen is an associative table of associative tables with for each supported OS a table that can be used to determine the generic architecture for every CPU. First level: The keys are the yyyymm versions of toolchains, the values the matching associative table. These toolchain versions are \"starting from\", so not every toolchain needs to be specified. Second level: Associative table with as keys the CPU names and as the value the generic CPU type for this CPU, or nil if it is already a generic one. Example: ClusterMod_map_cpu_to_gen = { [ '200000' ] = { [ 'zen3' ] = 'x86_64' , [ 'zen2' ] = 'x86_64' , [ 'skylake' ] = 'x86_64' , [ 'broadwell' ] = 'x86_64' , [ 'ivybridge' ] = 'x86_64' , [ 'x86_64' ] = nil , } } ClusterMod_reduce_cpu ClusterMod_reduce_cpu is an associative table of associative tables with for each supported OS a table that can be used to determine a compatible but less capable version of the CPU, until we end at the generic architectures. If the key is a generic architecture, the value also has to be a generic architecture, or 'nil' if the tree/chain of architectures ends there. First level: The keys are the yyyymm versions of toolchains, the values the matching associative table. These toolchain versions are \"starting from\", so not every toolchain needs to be specified. Second level: Associative table with as keys the CPU names and as the value the next compatible but less capable architecture, i.e., all software for the CPU as value should also run on the CPU as key but not always the other way around. For each stack in ClusterMod_SystemTable, these reduction rules have to be compatible with the matching ones in ClusterMod_reduce_top_arch. I.e., if somehow CPU1-Accel1 in ClusterMod_reduce_top_arch reduces to CPU2-Accel2 then it must also be possible to reduce CPU1 to CPU2 (in one or more steps) using the rules specified in the ClusterMod_reduce_top_arch table below. Example: ClusterMod_reduce_cpu = { [ '200000' ] = { [ 'zen3' ] = 'zen2' , [ 'zen2' ] = 'broadwell' , [ 'broadwell' ] = 'ivybridge' , [ 'ivybridge' ] = 'x86_64' , [ 'x86_64' ] = nil , }, } ClusterMod_reduce_top_arch ClusterMod_reduce_top_arch is an associative table of associative tables with for each stack version a table that can be used to walk a chain of compatible but less specific architectures when looking for an architecture that is supported for a particular version of a software stack. The version is given in six-digit number format and is a \"from then on\" version. As we forsee that this may change in incompatible ways in the future, there is a level that indexes with a yyyymm starting version of the software stacks. First level: The keys are the yyyymm versions of software stacks, the values the matching associative table. These toolchain versions are \"starting from\", so not every toolchain needs to be specified. Second level: Associative table with as keys the parten CPU/GPU architecture string and as value the child-GPU/architecture string or sub-architecture, or nil if it is the top (= most generic) architecture. Example: ClusterMod_reduce_top_arch = { [ '200000' ] = { [ 'zen2-ampere' ] = 'zen2-noaccel' , [ 'zen2-arcturus' ] = 'zen2-noaccel' , [ 'zen2-noaccel' ] = 'broadwell-noaccel' , [ 'skylake-aurora1' ] = 'skylake-noaccel' , [ 'skylake-noaccel' ] = 'broadwell-noaccel' , [ 'broadwell-noaccel' ] = 'ivybridge-noaccel' , [ 'broadwell-P5000' ] = 'broadwell-noaccel' , [ 'broadwell-pascal' ] = 'broadwell-noaccel' , [ 'ivybridge-noaccel' ] = 'x86_64' , [ 'x86_64' ] = nil , }, } ClusterMod_optarch ClusterMod_optarch is an associative table of associative tables with for each stack version and for a selection of architectures the optarch string for EasyBuild. The version is given in six-digit number format and is a \"from then on\" version. There is no need to specify an entry in the table if no special value for optarch is needed for that architecture. Example: ClusterMod_optarch = { [ '200000' ] = { [ 'zen3-noaccel' ] = 'Intel:march=core-avx2 -mtune=core-avx2' , [ 'zen2-ampere' ] = 'Intel:march=core-avx2 -mtune=core-avx2' , [ 'zen2-arcturus' ] = 'Intel:march=core-avx2 -mtune=core-avx2' , [ 'zen2-noaccel' ] = 'Intel:march=core-avx2 -mtune=core-avx2' , [ 'x86_64' ] = 'Intel:march=core-avx-i -mtune=core-avx-i' , }, } SitePackage_map_toolchain.lua Data structures Uses ClusterMod_toolchain_map from etc/SystemDefinition.lua Routines map_toolchain : Returns the matching yyyymm toolchain for any toolchain. The input argument can be either a yyyy.mm toolchain version (in which case the routine simply returns that version without the dot) or a yyyy[a|b] version in which case the routine uses the ClusterMod_toolchain_map to compute the matching yyyymm version or falls back to a default rule (where a becomes 01 and b becomes 07). get_versionedfile : Finds the most recent file (in terms of version encoded in the name of the file) with version not older than a given toolchain, with the file matching a particular pattern given as a directory, part before the version and part after the version. The version can be in any format supported by map_toolchain : yyyy[a|b], yyyy.mm, yyyymm, system . SitePackage_arch_hierarchy.lua Data structures From other files Uses ClusterMod_map_arch_hierarchy from etc/SystemDefinition.lua . map_os_long_to_short map_os_long_to_short maps long names of the OS to their short equivalent (name does not include the version). Example: map_os_long_to_short = { [ 'redhat' ] = 'RH' , } map_cpu_long_to_short map_cpu_long_to_short maps the long CPU name to their short equivalent. Example: map_cpu_long_to_short = { [ 'x86_64' ] = 'x86_64' , [ 'zen3' ] = 'zen3' , [ 'zen2' ] = 'zen2' , [ 'ivybridge' ] = 'IVB' , [ 'broadwell' ] = 'BRW' , [ 'skylake' ] = 'SKLX' , } map_accel_long_to_short map_accel_long_to_short maps the accelerator architecture from their long name to their short equivalent. Example: map_accel_long_to_short = { [ 'noaccel' ] = 'host' , [ 'P5000' ] = 'NVGP61GL' , [ 'pascal' ] = 'NVCC60' , [ 'ampere' ] = 'NVCC80' , [ 'arcturus' ] = 'GFX908' , [ 'aurora1' ] = 'NEC1' , } Routines Discover the architecture hierarchy get_osarchs : For a given OS (long name + version) and architecture (long name, CPU + optionally accelerator), return a table which also includes the parent and potentially grandparent of the architecture for the given OS. The order in the table is from the least generic architecture to the most generic one. get_osarchs_reverse : Same as get_osarchs , but now with the most generic one first and least generic one (the one used as the argument of the function) last. Mapping between different formats of names map_long_to_short : Map full long name (OS+version-CPU-accelerator) to the equivalent short name. It also works for names that do not include the accelerator. map_short_to_long : Map full short name (OS+version-CPU-accelerator) to the equivalent long name. It also works for names that do not include the accelerator. Extracting parts from the os-cpu-accelerator strings: extract_os : Extracts the first part of the os-cpu-acceleartor argument extract_cpu : Extracts the second part of the os-cpu-accelerator argument extract_accel : Extracts the third part of the os-cpu-accelerator argument, or returns nil if there is no accelerator part in the argument. extract_arch : Extracts the second and (optional) third part of the os-cpu-accelerator argument, i.e., returns cpu-accelerator or just cpu if there is no accelerator part. extract_cpu_from_arch : Extracts the first part of the cpu-accelerator argument extract_accel_from_arch : Extracts the second part of the cpu-accelerator argument Computing matching architectures in software stacks get_stack_osarch_current( stack_version ) Input argument: stack_version : Version of the calcua stack, can be system . Return value: The architecture of the current node with long names and in a format compatible with the indicated software stack (so taking into account the hierarchy types 2L or 3L). get_stack_generic( clusterarch, stack_version ) : Compute the most generic architecture for the given version of the CalcUA stack on the given clusterarch architecture. The clusterarch argument has to be in the long format compatible with the CalcUA stack version. get_stack_generic_current( stack_version ) : Compute the most generic architecture for the given version of the CalcUA stack on the current architecture. get_stack_top( osarch, stack_version ) : Input arguments: osarch : os and architecture with long names and in a format compatible with the indicated version of the software stack (so respecting the hierarchy types 2L or 3L). stack_version : Version of the calcua stack, can be system. Return value: The most specific os-architecture for the current node in the indicated version of the CalcUA software stacks. get_stack_matchingarch( osarch, reduce_stack_version, stack_version ) : Input arguments: osarch : os and architecture with long names and in a format compatible with the indicated version of the software stack (so respecting the hierarchy types 2L or 3L). reduce_stack_version : Stack version to use for the reduction rules of both os-CPU-accel and os-CPU names. stack_version : Version of the calcua stack, can be system for which the best matching architecture should be returned. If stack_version is a 3L hierarchy, this can be a middle level one if osarch is also a middle level name (which implies that reduce_stack_version is also a 3L hierarchy) Return value: The most specific os-architecture for the current node in the indicated version of the CalcUA software stacks. The precise rules for the matching are very tricky. The thing that makes this routine very tricky is that it will also be used in middle level arch modules for 3L stacks, and that there also one must be able to find a good module in the system stack which may be 2L or 3L. Cases: - osarch is of type OS-generic CPU. As we currently have no rules to reduce generic CPUs to an even less capable generic one, we produce nil if osarch is not supported by stack_version and return osarch otherwise. - osarch is of type OS-CPU, i.e., a middle level architecture for a 3L hierarchy. This implies that reduce_stack_version must be a 3L stack. There are now 2 options: - `stack_version` is also a 3L hierarchy: We use the reduction rules for CPUs for `reduce_stack_version` to find a middle level or bottom/generic level supported by `stack_version`. - `stack_version` is a 2L hierarchy: We use the mapping to generic CPU defined by `ClusterMod_map_cpu_to_gen` for `reduce_stackversion` to find the matching generic CPU and then continue using the CPU chaining rules defined by osarch is of type os-generic CPU: We follow the CPU reduction path for reduce_stack_version to see if we can find a match in the generic architectures supported by stack_version . Computing directories get_system_module_dir( osarch, stack_name, stack_version ) : Compute the system module directory from the three input arguments: long os-and-architecture name, stack name and stack version. The directory name is an absolute path. Note system in the name does not denote the system stack but the whole system installation, versus the user installation. get_system_module_dirs( osarch, stack_name, stack_version ) : Compute the system module directory hierarchy from the three input arguments: long os-and-architecture name, stack name and stack version. The directory names returned are absolute paths, with the most generic one first. get_user_module_dir( osarch, stack_name, stack_version ) : Similar to get_system_module_dir but then for the modules in the user module tree. get_user_module_dirs( osarch, stack_name, stack_version ) : Similar to get_system_module_dirs but then for the modules in the user module tree. get_system_inframodule_dir( osarch, stack_name, stack_version ) : Get the infrastructure module directory for the given architecture, stack name and stack version. The function returns the absolute path: The system installation root is obtained via the get_system_install_root function. get_system_SW_dir( osarch, stack_name, stack_version ) : Get the system software installation directory for the given architecture, stack name and stack version. The function returns the absolute path: The system installation root is obtained via the get_system_install_root function. get_user_SW_dir( osarch, stack_name, stack_version ) : Get the user software installation directory for the given architecture, stack name and stack version. The function returns the absolute path: The user installation root is obtained via the get_user_install_root function. get_system_EBrepo_dir( osarch, stack_name, stack_version ) : Get the system EasyBuild repo directory for the given architecture, stack name and stack version. The function returns the absolute path; the system installation root is obtained via the get_system_install_root function. get_system_EBrepo_dirs( osarch, stack_name, stack_version ) : Get all relevant system EBrepo directories for the given architecture, stack name and stack version, so including the more generic architectures that are relevant for that stack, with the most generic one first. The list should match with the list of module directories generated by get_system_module_dirs . get_user_EBrepo_dir( osarch, stack_name, stack_version ) : Get the user EasyBuild repo directory for the given architecture, stack name and stack version. The function returns the absolute path; the user installation root is obtained via the get_user_install_root function. get_user_EBrepo_dirs( osarch, stack_name, stack_version ) : Get the user Easybuild repo directories for the given architecture, stack name and stack version, so including the more generic architectures that are relevant for that stack, with the most generic one first. The list should match with the list of module directories generated by get_user_module_dirs . Miscellaneous functions get_EasyBuild_version( stack_version ) : Returns the version of EasyBuild for the stack as determined by ClusterMod_SystemTable . get_stack_subarchs( osarch, stack_version ) : Compute a list containing the given osarch and its subarchs in the hierarchy of the naming scheme for the stack. So the list can be at most 3 elements long. The most generic one is at the front of the list. This is a helper function to get_system_module_dirs . populate_cache_subarchs( stack_version ) : Populate a part of the cache variable ClusterMod_cache_subarchs initialized in SitePackage_helper.lua with the other helper variables that are used throughout. For each stack version, the cache variable will return true for each valid architecture string in the software architecture hierarchy for that stack version. SitePackage_system_info.lua Data structures This file needs several data structures to map properties detected on the system to the actual OS, CPU and accelerator names used in the module system. Note that in the current implementation we maintain mappings to both the long and the short names, and this has to be consistent with the various map_* tables in SitePackage_arch_hierarchy.lua . Mappings to long names cpustring_to_longtarget is the mapping from the CPU string that can be found in /proc/cpuinfo to the names that are used in the module system. Several CPU strings can map to the same target. Example: local cpustring_to_longtarget = { AuthenticAMD_23_49 = 'zen2' , GenuineIntel_6_62 = 'ivybridge' , GenuineIntel_6_79 = 'broadwell' , GenuineIntel_6_85 = 'skylake' , } osname_to_longos is the mapping from the OS names are reported in /etc/os-release in the NAME field to the names used in the module system. Multiple values can map onto the same OS in the stack, e.g., all Red Hat compatible OSes map to a single name. Example: local osname_to_longos = { CentOS_Linux = 'redhat' , Rocky_Linux = 'redhat' , } accelerator_to_longacc is a more tricky table. It is used in the mapping from data read from the output of lspci to accelerator names, but the keys are not currently the values read with lspci . These values are hard coded in the routine that does detect the accelerator. Example: local accelerator_to_longacc = { AMD_MI100 = 'arcturus' , NVIDIA_GA100 = 'ampere' , NVIDIA_GP100 = 'pascal' , NVIDIA_GP104 = 'P5000' , NEC_aurora1 = 'aurora1' , } Mappings to short names The structure of each of these associative tables is completely equivalent to their long names equivalent, but using the short names as the value. In principle they could be computed from the long names variants utilising the mapping tables from long to short names as defined in SitePackage_arch_hierarchy.lua . cpustring_to_shorttarget osname_to_shortos accelerator_to_shoracc Other data structures os_version_type : Indicates if for the OS we should use the major version only or a major.minor version to distinguish architectures for the software stack. The need grew out of a difference between CentOS which only reports the major version in /etc/os-release in the VERSION_ID field and Rocky Linux which reports a major.minor version, while we still use only the major version in the software stack. Example local os_version_type = { CentOS_Linux = 'major' , Rocky_Linux = 'major' , } Routines get_hostname() Request the name of the host. The routine calls /bin/hostname . get_clustername() Returns the name of the cluster as defined in the system definition through the ClusterMod_ClusterName variable. get_stackname() Returns the name of the primary software stack as defined in the system definition through the ClusterMod_StackName variable. get_cpu_info() Returns the CPU string derived from the data in /proc/cpuinfo , by combining information from the vendor_id , family and model lines (separated by an underscore). get_os_info() returns 2 values: name and version of the OS. The name is extracted from the NAME line of /etc/os-release , the version from the VERSION_ID line but it may be converted from major.minor to major format if told so by the os_version_type table. get_accelerator_info Extracts the accelerator type, returning nil if no accelerator is found. The names returned are the long accelerator names used in the data structures. Current return value: AMD_MI100 (vaughan AMD Instinct nodes) NVIDIA_GP100 (leibniz Pascal nodes) NVIDIA_GP104 (leibniz visualization node) NVIDIA_GA100 (vaughan Ampere node) NEC_aurora1 (leibniz Aurora node) get_cluster_osarch() get_clusterarch_longosarch returns the cluster architecture in the os-cpu-accelerator format with long names, e.g., redhat8-zen2-noaccel or redhat8-skylake-aurora1 . This is the format that in our naming conventions would be denoted as osarch . get_clusterarch() This function may not be needed in the final implementation and may be eliminated in favor of get_cluster_osarch() . get_clusterarch returns the cluster architecture in four possible formats for the module system. It is then to the module system to select which one of the four it needs for which purpose (and that depends on the hierarchy field in ClusterMod_SystemProperties in /etc/SysteDefinition.lua ). The four formats are two with long names and two with short names, each time with three components or with only two components if there is no accelerator. Return values: Short minimal name, i.e., no -host is added for nodes without accelerator. Long minimal name, i.e., no -noaccel is added for nodes without accelerator. Short maximal name, with -host added for nodes without accelerator. This is the format that in our naming conventions would be denoted as short_osarch . Long maximal name, with -noaccel added for nodes without accelerator e.g., RH8-zen2, redhat8-zen2, RH8-zen2-host, redhat8-zen2-noaccel or RH8-SKLX-NEC1, redhat8-skylake-aurora1, RH8-SKLX-NEC1, redhat8-skylake-aurora1 . This is the format that in our naming conventions would be denoted as osarch . get_fullos() Returns the long OS name including the version (so the first component of the formats with long names of get_clusterarch ) Example return value: redhat8 on systems with CentOS 8.x or Rocky Linux 8.x. get_system_install_root() Get the root of the system installation via etc/SoftwareStack.lua . This function is needed as modules have no direct access to the variables defined in that file. get_user_install_root() Compute the directory for the EasyBuild user installation, or nil if that is explicitly turned off by setting EBU_USER_PREFIX to an empty string. get_systemrepo_modules() Returns the location of the repository with the module system code (the repo_modules variables from etc/SoftwareStack.lua ). get_systemrepo_easybuild() Returns the location of the repository with the module system code (the repo_easybuild variables from etc/SoftwareStack.lua ). SitePackage_helper.lua This file defines data structures derived from other data structures mentioned earlier on this page, which implies that the order of dofile commands is important as the initialisation code is executed when the file is included. Data structures ClusterMod_sorted_archmap_keys This data structure is a sorted list of the level 1 keys used in the ClusterMod_map_arch_hierarchy data structure. Its main purpose is to speed up a search routine in this file, to avoid always recomputing that data. TODO: GET RID OF THIS STRUCTURE ClusterMod_sorted_cputogen_keys This data structure is a sorted list of the level 1 keys used in the ClusterMod_map_cpu_to_gen data structure. Its main purpose is to speed up a search routine in this file, to avoid always recomputing that data. ClusterMod_sorted_reducecpu_keys This data structure is a sorted list of the level 1 keys used in the ClusterMod_reduce_cpu data structure. Its main purpose is to speed up a search routine in this file, to avoid always recomputing that data. ClusterMod_sorted_toparchreduction_keys This data structure is a sorted list of the level 1 keys used in the ClusterMod_reduce_top_arch data structure. Its main purpose is to speed up a search routine in this file, to avoid always recomputing that data. ClusterMod_sorted_optarch_keys This data structure is a sorted list of the level 1 keys used in the ClusterMod_optarch data structure. Its main purpose is to speed up a search routine in this file, to avoid always recomputing that data. Routines get_matching_archmap_key( version ) : For a given numeric (i.e., yyyymm) version, returns the largest key in ClusterMod_map_arch_hierarchy not larger than the given version. get_matching_defcpu_key( version ) : For a given numeric (i.e., yyyymm) version, returns the largest key in ClusterMod_map_def_cpu not larger than the given version. get_matching_cputogen_key( version ) : For a given numeric (i.e., yyyymm) version, returns the largest key in ClusterMod_map_cpu_to_gen not larger than the given version. get_matching_reducecpu_key( version ) : For a given numeric (i.e., yyyymm) version, returns the largest key in ClusterMod_reduce_cpu not larger than the given version. get_matching_toparchreduction_key( version ) : For a given numeric (i.e., yyyymm) version, returns the largest key in ClusterMod_reduce_top_arch not larger than the given version. get_matching_optarch_key( version ) : For a given numeric (i.e., yyyymm) version, returns the largest key in ClusterMod_reduce_top_arch not larger than the given version. is_Stack_SystemTable : Check if a given stack version corresponds to a key in ClusterMod_SystemTable . We have to do this through a function that is then exported to the sandbox as module files do not have access to the data itself. The main purpose of this function is simply to give more precise error messages in case the data structures aren't updated properly after installing a new toolchain. This problem should not occur as the routines to prepare the stack would fail themselves, but you'll never know. mkDir : Create a directory, using the Lua lfs package. It really has the effect effect of mkdir -p , so it can create multiple levels from the given directory if needed.","title":"The SitePackage.lua file and its included LUA files"},{"location":"SitePackage/#the-sitepackagelua-file-and-its-included-routines","text":"SitePackage_map_toolchain.lua : Mapping regular toolchain names onto yyy.mm for comparisons to find matching versions of some files. SitePackage_arch_hierarchy.lua : Routines to work with the hierarchy of architectures and map between long and short names. SitePackage_system_info.lua : Routines to gather information about the system. SitePackage_helper.lua : Other routines and data structures. Some of the data structures, in particular those that need to be extended with each new toolchain and really define the setup of the system, are not in the LMOD subdirectory but instead in the file etc/SystemDefinition.lua .","title":"The SitePackage.lua file and its included routines"},{"location":"SitePackage/#naming-conventions","text":"Full 3-component architecture: osarch (where possible) Full 2-component architecture without OS: arch (where possible) Supported OS as name + version: os 2-component architecture (CPU + accelerator): modarch Note that we discussed a 2-level scheme where we always use the shortest name possible (hence avoiding -noaccel ) but this is not implemented as we feel it may create confusion and as it also complicates the implementation.","title":"Naming conventions"},{"location":"SitePackage/#etcsystemdefinitionlua","text":"TODO: Some clean-up. Some of these tables could be generated automatically in SitePackage_helper.lua ?","title":"etc/SystemDefinition.lua"},{"location":"SitePackage/#clustermod_nodetypes","text":"ClusterMod_NodeTypes is simply n array of nodes in the system, specified using the long os-CPU-accelerator names. As this is a description of the current hardware in the cluster, it is not for a specific version of the software stack. The table is used to produce output for debug purposes of this configuration file, e.g., to list which software stacks for which architectures will be available on which node types. Note that one should distinguish between the generic processor types and the real processor types. No node type in the table below should have a generic processor type in its name as that would cause problems with the 3-level software architecture schemes (and make coding more difficult even for the 2-level software architecture scheme). Example: At the time of writing, the CalcUA cluster description would have been: ClusterMod_NodeTypes = { 'redhat7-ivybridge-noaccel' , 'redhat7-broadwell-noaccel' , 'redhat8-broadwell-noaccel' , 'redhat8-broadwell-pascal' , 'redhat8-broadwell-P5000' , 'redhat8-skylake-noaccel' , 'redhat8-skylake-aurora1' , 'redhat8-zen2-noaccel' , 'redhat8-zen2-ampere' , 'redhat8-zen2-arcturus' , }","title":"ClusterMod_NodeTypes"},{"location":"SitePackage/#clustermod_systemtable","text":"ClusterMod_SystemTable defines the whole structure of the software tree, including the manually installed software and system-wide EasyBuild managed software. Note that the table will be completed automatically with more generic os-cpu-accelerator architecture strings based on the other tables in this file. All names used should be for the 3L scheme. However, the middle level should not be used for versions that will use a 2L_long naming scheme. ClusterMod_SystemTable = { [ 'system' ] = { [ 'redhat7' ] = { 'x86_64' , }, [ 'redhat8' ] = { 'x86_64' , }, }, [ 'manual' ] = { [ 'redhat7' ] = { 'x86_64' , }, [ 'redhat8' ] = { 'x86_64' , }, }, [ '2020a' ] = { [ 'redhat7' ] = { 'ivybridge-noaccel' , 'broadwell-noaccel' , }, [ 'redhat8' ] = { 'zen2-noaccel' , 'skylake-noaccel' , } }, [ '2021b' ] = { [ 'redhat7' ] = { 'ivybridge-noaccel' , }, [ 'redhat8' ] = { 'broadwell-noaccel' , 'zen2-noaccel' , 'skylake-noaccel' , } }, } The structure is a table of tables. The keys on the first level are the versions of the software stacks, with two special version: system and manual . The keys on the second level are the long OS versions (name+version as used in the architecture identifiers) The values are then the CPU architecture + accelerator for each software stack + OS combo. Only the most specific architecture needs to be given, the others are derived automatically from other tables.","title":"ClusterMod_SystemTable"},{"location":"SitePackage/#clustermod_systemproperies","text":"ClusterMod_SystemProperies adds additional information for each partition that is not contained in ClusterMod_SystemTable . It is a table of tables, the first index is the name of the calcua stack or manual for the pseudo-stack of manually installed software. The table for each software stack has the following entries: EasyBuild : Default version of EasyBuild for this stack. Should not be defined for the manual stack. hierarchy : Type of architecture hierarchy used for this software stack. Currently only the first option is implemented: 2L : two levels, the least generic level always includes an accelerator field 3L : 3 levels in the architecture hierarchy.","title":"ClusterMod_SystemProperies"},{"location":"SitePackage/#clustermod_clustermap","text":"ClusterMod_ClusterMap contains for each version of the calcua toolchains, including the dummy system version, a mapping from cluster names to os-architecture strings. Each should be the topmost supported architecture for a particular node type. ClusterMod_ClusterMap = { [ 'system' ] = { [ 'hopper' ] = 'redhat7-x86_64' , [ 'leibniz' ] = 'redhat8-x86_64' , [ 'leibniz-skl' ] = 'redhat8-x86_64' , [ 'vaughan' ] = 'redhat8-x86_64' , }, [ '2020a' ] = { [ 'hopper' ] = 'redhat7-ivybridge-noaccel' , [ 'leibniz' ] = 'redhat7-broadwell-noaccel' , [ 'leibniz-skl' ] = 'redhat8-skylake-noaccel' , [ 'vaughan' ] = 'redhat8-zen2-noaccel' , }, [ '2021b' ] = { [ 'hopper' ] = 'redhat7-ivybridge-noaccel' , [ 'leibniz' ] = 'redhat8-broadwell-noaccel' , [ 'leibniz-skl' ] = 'redhat8-skylake-noaccel' , [ 'vaughan' ] = 'redhat8-zen2-noaccel' , }, } ClusterMod_ClusterMap is an associative table-of-tables. On the first level, the keys are the various versions of the software stack visible to users (and can include system ) On the second level, the keys are the names of the clusters that we want to map on a particular architecture. The values are then the architecture strings, in the formats specified in ClusterMod_SystemProperties .","title":"ClusterMod_ClusterMap"},{"location":"SitePackage/#clustermod_toolchain_map","text":"ClusterMod_toolchain_map is an associative table, with the yyyy[a|b] toolchains as the keys and the matching yyyymm value as the value (note: no dot, not yyyy.mm) Example: ClusterMod_toolchain_map = { [ 'system' ] = '200000' , [ 'manual' ] = '200000' , [ '2020a' ] = '202001' , [ '2020b' ] = '202007' , [ '2021a' ] = '202101' , [ '2021b' ] = '202107' , [ '2022a' ] = '202201' , }","title":"ClusterMod_toolchain_map"},{"location":"SitePackage/#clustermod_map_arch_hierarchy","text":"ClusterMod_map_arch_hierarchy is an associative table of associative tables with for each supported OS a table that can be used to determine the parent of every CPU/accelerator architecture. First level: The keys are the yyyymm versions of toolchains, the values the matching associative table. These toolchain versions are \"starting from\", so not every toolchain needs to be specified. Second level: Associative table with as keys the CPU/GPU architecture string and as value the parent GPU/architecture string, or nil if it is the top (= most generic) architecture. Example: ClusterMod_map_arch_hierarchy = { -- We start with a 2-level map [ '200000' ] = { [ 'zen2-noaccel' ] = 'x86_64' , [ 'zen2-ampere' ] = 'x86_64' , [ 'zen2-arcturus' ] = 'x86_64' , [ 'broadwell-noaccel' ] = 'x86_64' , [ 'broadwell-P5000' ] = 'x86_64' , [ 'broadwell-pascal' ] = 'x86_64' , [ 'skylake-noaccel' ] = 'x86_64' , [ 'skylake-aurora1' ] = 'x86_64' , [ 'ivybridge-noaccel' ] = 'x86_64' , [ 'x86_64' ] = nil , } } TODO: Can we get rid of the above data structure?","title":"ClusterMod_map_arch_hierarchy"},{"location":"SitePackage/#clustermod_def_cpu","text":"ClusterMod_def_cpu is an associative table of associative tables defining the CPU architectures and whether they are generic or not. First level: The keys are the yyyymm versions of toolchains, the values the matching associative table. These toolchain versions are \"starting from\", so not every toolchain needs to be specified. Second level: Associative table with as keys the CPU type (long names) and as value true for generic CPU architectures and false otherwise. The map is versioned, but do expect problems with finding the right version of the system stack for a regular stack if all of a sudden a regular CPU would become generic or vice-versa, so in practice it is very likely only one version will ever be needed as it can be safely extended with new types. Example: ClusterMod_def_cpu = { [ 'zen4' ] = false , [ 'zen3' ] = false , [ 'zen2' ] = false , [ 'skylake' ] = false , [ 'broadwell' ] = false , [ 'ivybridge' ] = false , [ 'x86_64' ] = true , }","title":"ClusterMod_def_cpu"},{"location":"SitePackage/#clustermod_map_cpu_to_gen","text":"ClusterMod_map_cpu_to_gen is an associative table of associative tables with for each supported OS a table that can be used to determine the generic architecture for every CPU. First level: The keys are the yyyymm versions of toolchains, the values the matching associative table. These toolchain versions are \"starting from\", so not every toolchain needs to be specified. Second level: Associative table with as keys the CPU names and as the value the generic CPU type for this CPU, or nil if it is already a generic one. Example: ClusterMod_map_cpu_to_gen = { [ '200000' ] = { [ 'zen3' ] = 'x86_64' , [ 'zen2' ] = 'x86_64' , [ 'skylake' ] = 'x86_64' , [ 'broadwell' ] = 'x86_64' , [ 'ivybridge' ] = 'x86_64' , [ 'x86_64' ] = nil , } }","title":"ClusterMod_map_cpu_to_gen"},{"location":"SitePackage/#clustermod_reduce_cpu","text":"ClusterMod_reduce_cpu is an associative table of associative tables with for each supported OS a table that can be used to determine a compatible but less capable version of the CPU, until we end at the generic architectures. If the key is a generic architecture, the value also has to be a generic architecture, or 'nil' if the tree/chain of architectures ends there. First level: The keys are the yyyymm versions of toolchains, the values the matching associative table. These toolchain versions are \"starting from\", so not every toolchain needs to be specified. Second level: Associative table with as keys the CPU names and as the value the next compatible but less capable architecture, i.e., all software for the CPU as value should also run on the CPU as key but not always the other way around. For each stack in ClusterMod_SystemTable, these reduction rules have to be compatible with the matching ones in ClusterMod_reduce_top_arch. I.e., if somehow CPU1-Accel1 in ClusterMod_reduce_top_arch reduces to CPU2-Accel2 then it must also be possible to reduce CPU1 to CPU2 (in one or more steps) using the rules specified in the ClusterMod_reduce_top_arch table below. Example: ClusterMod_reduce_cpu = { [ '200000' ] = { [ 'zen3' ] = 'zen2' , [ 'zen2' ] = 'broadwell' , [ 'broadwell' ] = 'ivybridge' , [ 'ivybridge' ] = 'x86_64' , [ 'x86_64' ] = nil , }, }","title":"ClusterMod_reduce_cpu"},{"location":"SitePackage/#clustermod_reduce_top_arch","text":"ClusterMod_reduce_top_arch is an associative table of associative tables with for each stack version a table that can be used to walk a chain of compatible but less specific architectures when looking for an architecture that is supported for a particular version of a software stack. The version is given in six-digit number format and is a \"from then on\" version. As we forsee that this may change in incompatible ways in the future, there is a level that indexes with a yyyymm starting version of the software stacks. First level: The keys are the yyyymm versions of software stacks, the values the matching associative table. These toolchain versions are \"starting from\", so not every toolchain needs to be specified. Second level: Associative table with as keys the parten CPU/GPU architecture string and as value the child-GPU/architecture string or sub-architecture, or nil if it is the top (= most generic) architecture. Example: ClusterMod_reduce_top_arch = { [ '200000' ] = { [ 'zen2-ampere' ] = 'zen2-noaccel' , [ 'zen2-arcturus' ] = 'zen2-noaccel' , [ 'zen2-noaccel' ] = 'broadwell-noaccel' , [ 'skylake-aurora1' ] = 'skylake-noaccel' , [ 'skylake-noaccel' ] = 'broadwell-noaccel' , [ 'broadwell-noaccel' ] = 'ivybridge-noaccel' , [ 'broadwell-P5000' ] = 'broadwell-noaccel' , [ 'broadwell-pascal' ] = 'broadwell-noaccel' , [ 'ivybridge-noaccel' ] = 'x86_64' , [ 'x86_64' ] = nil , }, }","title":"ClusterMod_reduce_top_arch"},{"location":"SitePackage/#clustermod_optarch","text":"ClusterMod_optarch is an associative table of associative tables with for each stack version and for a selection of architectures the optarch string for EasyBuild. The version is given in six-digit number format and is a \"from then on\" version. There is no need to specify an entry in the table if no special value for optarch is needed for that architecture. Example: ClusterMod_optarch = { [ '200000' ] = { [ 'zen3-noaccel' ] = 'Intel:march=core-avx2 -mtune=core-avx2' , [ 'zen2-ampere' ] = 'Intel:march=core-avx2 -mtune=core-avx2' , [ 'zen2-arcturus' ] = 'Intel:march=core-avx2 -mtune=core-avx2' , [ 'zen2-noaccel' ] = 'Intel:march=core-avx2 -mtune=core-avx2' , [ 'x86_64' ] = 'Intel:march=core-avx-i -mtune=core-avx-i' , }, }","title":"ClusterMod_optarch"},{"location":"SitePackage/#sitepackage_map_toolchainlua","text":"","title":"SitePackage_map_toolchain.lua"},{"location":"SitePackage/#data-structures","text":"Uses ClusterMod_toolchain_map from etc/SystemDefinition.lua","title":"Data structures"},{"location":"SitePackage/#routines","text":"map_toolchain : Returns the matching yyyymm toolchain for any toolchain. The input argument can be either a yyyy.mm toolchain version (in which case the routine simply returns that version without the dot) or a yyyy[a|b] version in which case the routine uses the ClusterMod_toolchain_map to compute the matching yyyymm version or falls back to a default rule (where a becomes 01 and b becomes 07). get_versionedfile : Finds the most recent file (in terms of version encoded in the name of the file) with version not older than a given toolchain, with the file matching a particular pattern given as a directory, part before the version and part after the version. The version can be in any format supported by map_toolchain : yyyy[a|b], yyyy.mm, yyyymm, system .","title":"Routines"},{"location":"SitePackage/#sitepackage_arch_hierarchylua","text":"","title":"SitePackage_arch_hierarchy.lua"},{"location":"SitePackage/#data-structures_1","text":"","title":"Data structures"},{"location":"SitePackage/#from-other-files","text":"Uses ClusterMod_map_arch_hierarchy from etc/SystemDefinition.lua .","title":"From other files"},{"location":"SitePackage/#map_os_long_to_short","text":"map_os_long_to_short maps long names of the OS to their short equivalent (name does not include the version). Example: map_os_long_to_short = { [ 'redhat' ] = 'RH' , }","title":"map_os_long_to_short"},{"location":"SitePackage/#map_cpu_long_to_short","text":"map_cpu_long_to_short maps the long CPU name to their short equivalent. Example: map_cpu_long_to_short = { [ 'x86_64' ] = 'x86_64' , [ 'zen3' ] = 'zen3' , [ 'zen2' ] = 'zen2' , [ 'ivybridge' ] = 'IVB' , [ 'broadwell' ] = 'BRW' , [ 'skylake' ] = 'SKLX' , }","title":"map_cpu_long_to_short"},{"location":"SitePackage/#map_accel_long_to_short","text":"map_accel_long_to_short maps the accelerator architecture from their long name to their short equivalent. Example: map_accel_long_to_short = { [ 'noaccel' ] = 'host' , [ 'P5000' ] = 'NVGP61GL' , [ 'pascal' ] = 'NVCC60' , [ 'ampere' ] = 'NVCC80' , [ 'arcturus' ] = 'GFX908' , [ 'aurora1' ] = 'NEC1' , }","title":"map_accel_long_to_short"},{"location":"SitePackage/#routines_1","text":"","title":"Routines"},{"location":"SitePackage/#discover-the-architecture-hierarchy","text":"get_osarchs : For a given OS (long name + version) and architecture (long name, CPU + optionally accelerator), return a table which also includes the parent and potentially grandparent of the architecture for the given OS. The order in the table is from the least generic architecture to the most generic one. get_osarchs_reverse : Same as get_osarchs , but now with the most generic one first and least generic one (the one used as the argument of the function) last.","title":"Discover the architecture hierarchy"},{"location":"SitePackage/#mapping-between-different-formats-of-names","text":"map_long_to_short : Map full long name (OS+version-CPU-accelerator) to the equivalent short name. It also works for names that do not include the accelerator. map_short_to_long : Map full short name (OS+version-CPU-accelerator) to the equivalent long name. It also works for names that do not include the accelerator.","title":"Mapping between different formats of names"},{"location":"SitePackage/#extracting-parts-from-the-os-cpu-accelerator-strings","text":"extract_os : Extracts the first part of the os-cpu-acceleartor argument extract_cpu : Extracts the second part of the os-cpu-accelerator argument extract_accel : Extracts the third part of the os-cpu-accelerator argument, or returns nil if there is no accelerator part in the argument. extract_arch : Extracts the second and (optional) third part of the os-cpu-accelerator argument, i.e., returns cpu-accelerator or just cpu if there is no accelerator part. extract_cpu_from_arch : Extracts the first part of the cpu-accelerator argument extract_accel_from_arch : Extracts the second part of the cpu-accelerator argument","title":"Extracting parts from the os-cpu-accelerator strings:"},{"location":"SitePackage/#computing-matching-architectures-in-software-stacks","text":"get_stack_osarch_current( stack_version ) Input argument: stack_version : Version of the calcua stack, can be system . Return value: The architecture of the current node with long names and in a format compatible with the indicated software stack (so taking into account the hierarchy types 2L or 3L). get_stack_generic( clusterarch, stack_version ) : Compute the most generic architecture for the given version of the CalcUA stack on the given clusterarch architecture. The clusterarch argument has to be in the long format compatible with the CalcUA stack version. get_stack_generic_current( stack_version ) : Compute the most generic architecture for the given version of the CalcUA stack on the current architecture. get_stack_top( osarch, stack_version ) : Input arguments: osarch : os and architecture with long names and in a format compatible with the indicated version of the software stack (so respecting the hierarchy types 2L or 3L). stack_version : Version of the calcua stack, can be system. Return value: The most specific os-architecture for the current node in the indicated version of the CalcUA software stacks. get_stack_matchingarch( osarch, reduce_stack_version, stack_version ) : Input arguments: osarch : os and architecture with long names and in a format compatible with the indicated version of the software stack (so respecting the hierarchy types 2L or 3L). reduce_stack_version : Stack version to use for the reduction rules of both os-CPU-accel and os-CPU names. stack_version : Version of the calcua stack, can be system for which the best matching architecture should be returned. If stack_version is a 3L hierarchy, this can be a middle level one if osarch is also a middle level name (which implies that reduce_stack_version is also a 3L hierarchy) Return value: The most specific os-architecture for the current node in the indicated version of the CalcUA software stacks. The precise rules for the matching are very tricky. The thing that makes this routine very tricky is that it will also be used in middle level arch modules for 3L stacks, and that there also one must be able to find a good module in the system stack which may be 2L or 3L. Cases: - osarch is of type OS-generic CPU. As we currently have no rules to reduce generic CPUs to an even less capable generic one, we produce nil if osarch is not supported by stack_version and return osarch otherwise. - osarch is of type OS-CPU, i.e., a middle level architecture for a 3L hierarchy. This implies that reduce_stack_version must be a 3L stack. There are now 2 options: - `stack_version` is also a 3L hierarchy: We use the reduction rules for CPUs for `reduce_stack_version` to find a middle level or bottom/generic level supported by `stack_version`. - `stack_version` is a 2L hierarchy: We use the mapping to generic CPU defined by `ClusterMod_map_cpu_to_gen` for `reduce_stackversion` to find the matching generic CPU and then continue using the CPU chaining rules defined by osarch is of type os-generic CPU: We follow the CPU reduction path for reduce_stack_version to see if we can find a match in the generic architectures supported by stack_version .","title":"Computing matching architectures in software stacks"},{"location":"SitePackage/#computing-directories","text":"get_system_module_dir( osarch, stack_name, stack_version ) : Compute the system module directory from the three input arguments: long os-and-architecture name, stack name and stack version. The directory name is an absolute path. Note system in the name does not denote the system stack but the whole system installation, versus the user installation. get_system_module_dirs( osarch, stack_name, stack_version ) : Compute the system module directory hierarchy from the three input arguments: long os-and-architecture name, stack name and stack version. The directory names returned are absolute paths, with the most generic one first. get_user_module_dir( osarch, stack_name, stack_version ) : Similar to get_system_module_dir but then for the modules in the user module tree. get_user_module_dirs( osarch, stack_name, stack_version ) : Similar to get_system_module_dirs but then for the modules in the user module tree. get_system_inframodule_dir( osarch, stack_name, stack_version ) : Get the infrastructure module directory for the given architecture, stack name and stack version. The function returns the absolute path: The system installation root is obtained via the get_system_install_root function. get_system_SW_dir( osarch, stack_name, stack_version ) : Get the system software installation directory for the given architecture, stack name and stack version. The function returns the absolute path: The system installation root is obtained via the get_system_install_root function. get_user_SW_dir( osarch, stack_name, stack_version ) : Get the user software installation directory for the given architecture, stack name and stack version. The function returns the absolute path: The user installation root is obtained via the get_user_install_root function. get_system_EBrepo_dir( osarch, stack_name, stack_version ) : Get the system EasyBuild repo directory for the given architecture, stack name and stack version. The function returns the absolute path; the system installation root is obtained via the get_system_install_root function. get_system_EBrepo_dirs( osarch, stack_name, stack_version ) : Get all relevant system EBrepo directories for the given architecture, stack name and stack version, so including the more generic architectures that are relevant for that stack, with the most generic one first. The list should match with the list of module directories generated by get_system_module_dirs . get_user_EBrepo_dir( osarch, stack_name, stack_version ) : Get the user EasyBuild repo directory for the given architecture, stack name and stack version. The function returns the absolute path; the user installation root is obtained via the get_user_install_root function. get_user_EBrepo_dirs( osarch, stack_name, stack_version ) : Get the user Easybuild repo directories for the given architecture, stack name and stack version, so including the more generic architectures that are relevant for that stack, with the most generic one first. The list should match with the list of module directories generated by get_user_module_dirs .","title":"Computing directories"},{"location":"SitePackage/#miscellaneous-functions","text":"get_EasyBuild_version( stack_version ) : Returns the version of EasyBuild for the stack as determined by ClusterMod_SystemTable . get_stack_subarchs( osarch, stack_version ) : Compute a list containing the given osarch and its subarchs in the hierarchy of the naming scheme for the stack. So the list can be at most 3 elements long. The most generic one is at the front of the list. This is a helper function to get_system_module_dirs . populate_cache_subarchs( stack_version ) : Populate a part of the cache variable ClusterMod_cache_subarchs initialized in SitePackage_helper.lua with the other helper variables that are used throughout. For each stack version, the cache variable will return true for each valid architecture string in the software architecture hierarchy for that stack version.","title":"Miscellaneous functions"},{"location":"SitePackage/#sitepackage_system_infolua","text":"","title":"SitePackage_system_info.lua"},{"location":"SitePackage/#data-structures_2","text":"This file needs several data structures to map properties detected on the system to the actual OS, CPU and accelerator names used in the module system. Note that in the current implementation we maintain mappings to both the long and the short names, and this has to be consistent with the various map_* tables in SitePackage_arch_hierarchy.lua .","title":"Data structures"},{"location":"SitePackage/#mappings-to-long-names","text":"cpustring_to_longtarget is the mapping from the CPU string that can be found in /proc/cpuinfo to the names that are used in the module system. Several CPU strings can map to the same target. Example: local cpustring_to_longtarget = { AuthenticAMD_23_49 = 'zen2' , GenuineIntel_6_62 = 'ivybridge' , GenuineIntel_6_79 = 'broadwell' , GenuineIntel_6_85 = 'skylake' , } osname_to_longos is the mapping from the OS names are reported in /etc/os-release in the NAME field to the names used in the module system. Multiple values can map onto the same OS in the stack, e.g., all Red Hat compatible OSes map to a single name. Example: local osname_to_longos = { CentOS_Linux = 'redhat' , Rocky_Linux = 'redhat' , } accelerator_to_longacc is a more tricky table. It is used in the mapping from data read from the output of lspci to accelerator names, but the keys are not currently the values read with lspci . These values are hard coded in the routine that does detect the accelerator. Example: local accelerator_to_longacc = { AMD_MI100 = 'arcturus' , NVIDIA_GA100 = 'ampere' , NVIDIA_GP100 = 'pascal' , NVIDIA_GP104 = 'P5000' , NEC_aurora1 = 'aurora1' , }","title":"Mappings to long names"},{"location":"SitePackage/#mappings-to-short-names","text":"The structure of each of these associative tables is completely equivalent to their long names equivalent, but using the short names as the value. In principle they could be computed from the long names variants utilising the mapping tables from long to short names as defined in SitePackage_arch_hierarchy.lua . cpustring_to_shorttarget osname_to_shortos accelerator_to_shoracc","title":"Mappings to short names"},{"location":"SitePackage/#other-data-structures","text":"os_version_type : Indicates if for the OS we should use the major version only or a major.minor version to distinguish architectures for the software stack. The need grew out of a difference between CentOS which only reports the major version in /etc/os-release in the VERSION_ID field and Rocky Linux which reports a major.minor version, while we still use only the major version in the software stack. Example local os_version_type = { CentOS_Linux = 'major' , Rocky_Linux = 'major' , }","title":"Other data structures"},{"location":"SitePackage/#routines_2","text":"","title":"Routines"},{"location":"SitePackage/#get_hostname","text":"Request the name of the host. The routine calls /bin/hostname .","title":"get_hostname()"},{"location":"SitePackage/#get_clustername","text":"Returns the name of the cluster as defined in the system definition through the ClusterMod_ClusterName variable.","title":"get_clustername()"},{"location":"SitePackage/#get_stackname","text":"Returns the name of the primary software stack as defined in the system definition through the ClusterMod_StackName variable.","title":"get_stackname()"},{"location":"SitePackage/#get_cpu_info","text":"Returns the CPU string derived from the data in /proc/cpuinfo , by combining information from the vendor_id , family and model lines (separated by an underscore).","title":"get_cpu_info()"},{"location":"SitePackage/#get_os_info","text":"returns 2 values: name and version of the OS. The name is extracted from the NAME line of /etc/os-release , the version from the VERSION_ID line but it may be converted from major.minor to major format if told so by the os_version_type table.","title":"get_os_info()"},{"location":"SitePackage/#get_accelerator_info","text":"Extracts the accelerator type, returning nil if no accelerator is found. The names returned are the long accelerator names used in the data structures. Current return value: AMD_MI100 (vaughan AMD Instinct nodes) NVIDIA_GP100 (leibniz Pascal nodes) NVIDIA_GP104 (leibniz visualization node) NVIDIA_GA100 (vaughan Ampere node) NEC_aurora1 (leibniz Aurora node)","title":"get_accelerator_info"},{"location":"SitePackage/#get_cluster_osarch","text":"get_clusterarch_longosarch returns the cluster architecture in the os-cpu-accelerator format with long names, e.g., redhat8-zen2-noaccel or redhat8-skylake-aurora1 . This is the format that in our naming conventions would be denoted as osarch .","title":"get_cluster_osarch()"},{"location":"SitePackage/#get_clusterarch","text":"This function may not be needed in the final implementation and may be eliminated in favor of get_cluster_osarch() . get_clusterarch returns the cluster architecture in four possible formats for the module system. It is then to the module system to select which one of the four it needs for which purpose (and that depends on the hierarchy field in ClusterMod_SystemProperties in /etc/SysteDefinition.lua ). The four formats are two with long names and two with short names, each time with three components or with only two components if there is no accelerator. Return values: Short minimal name, i.e., no -host is added for nodes without accelerator. Long minimal name, i.e., no -noaccel is added for nodes without accelerator. Short maximal name, with -host added for nodes without accelerator. This is the format that in our naming conventions would be denoted as short_osarch . Long maximal name, with -noaccel added for nodes without accelerator e.g., RH8-zen2, redhat8-zen2, RH8-zen2-host, redhat8-zen2-noaccel or RH8-SKLX-NEC1, redhat8-skylake-aurora1, RH8-SKLX-NEC1, redhat8-skylake-aurora1 . This is the format that in our naming conventions would be denoted as osarch .","title":"get_clusterarch()"},{"location":"SitePackage/#get_fullos","text":"Returns the long OS name including the version (so the first component of the formats with long names of get_clusterarch ) Example return value: redhat8 on systems with CentOS 8.x or Rocky Linux 8.x.","title":"get_fullos()"},{"location":"SitePackage/#get_system_install_root","text":"Get the root of the system installation via etc/SoftwareStack.lua . This function is needed as modules have no direct access to the variables defined in that file.","title":"get_system_install_root()"},{"location":"SitePackage/#get_user_install_root","text":"Compute the directory for the EasyBuild user installation, or nil if that is explicitly turned off by setting EBU_USER_PREFIX to an empty string.","title":"get_user_install_root()"},{"location":"SitePackage/#get_systemrepo_modules","text":"Returns the location of the repository with the module system code (the repo_modules variables from etc/SoftwareStack.lua ).","title":"get_systemrepo_modules()"},{"location":"SitePackage/#get_systemrepo_easybuild","text":"Returns the location of the repository with the module system code (the repo_easybuild variables from etc/SoftwareStack.lua ).","title":"get_systemrepo_easybuild()"},{"location":"SitePackage/#sitepackage_helperlua","text":"This file defines data structures derived from other data structures mentioned earlier on this page, which implies that the order of dofile commands is important as the initialisation code is executed when the file is included.","title":"SitePackage_helper.lua"},{"location":"SitePackage/#data-structures_3","text":"","title":"Data structures"},{"location":"SitePackage/#clustermod_sorted_archmap_keys","text":"This data structure is a sorted list of the level 1 keys used in the ClusterMod_map_arch_hierarchy data structure. Its main purpose is to speed up a search routine in this file, to avoid always recomputing that data. TODO: GET RID OF THIS STRUCTURE","title":"ClusterMod_sorted_archmap_keys"},{"location":"SitePackage/#clustermod_sorted_cputogen_keys","text":"This data structure is a sorted list of the level 1 keys used in the ClusterMod_map_cpu_to_gen data structure. Its main purpose is to speed up a search routine in this file, to avoid always recomputing that data.","title":"ClusterMod_sorted_cputogen_keys"},{"location":"SitePackage/#clustermod_sorted_reducecpu_keys","text":"This data structure is a sorted list of the level 1 keys used in the ClusterMod_reduce_cpu data structure. Its main purpose is to speed up a search routine in this file, to avoid always recomputing that data.","title":"ClusterMod_sorted_reducecpu_keys"},{"location":"SitePackage/#clustermod_sorted_optarch_keys","text":"This data structure is a sorted list of the level 1 keys used in the ClusterMod_optarch data structure. Its main purpose is to speed up a search routine in this file, to avoid always recomputing that data.","title":"ClusterMod_sorted_optarch_keys"},{"location":"SitePackage/#routines_3","text":"get_matching_archmap_key( version ) : For a given numeric (i.e., yyyymm) version, returns the largest key in ClusterMod_map_arch_hierarchy not larger than the given version. get_matching_defcpu_key( version ) : For a given numeric (i.e., yyyymm) version, returns the largest key in ClusterMod_map_def_cpu not larger than the given version. get_matching_cputogen_key( version ) : For a given numeric (i.e., yyyymm) version, returns the largest key in ClusterMod_map_cpu_to_gen not larger than the given version. get_matching_reducecpu_key( version ) : For a given numeric (i.e., yyyymm) version, returns the largest key in ClusterMod_reduce_cpu not larger than the given version. get_matching_toparchreduction_key( version ) : For a given numeric (i.e., yyyymm) version, returns the largest key in ClusterMod_reduce_top_arch not larger than the given version. get_matching_optarch_key( version ) : For a given numeric (i.e., yyyymm) version, returns the largest key in ClusterMod_reduce_top_arch not larger than the given version. is_Stack_SystemTable : Check if a given stack version corresponds to a key in ClusterMod_SystemTable . We have to do this through a function that is then exported to the sandbox as module files do not have access to the data itself. The main purpose of this function is simply to give more precise error messages in case the data structures aren't updated properly after installing a new toolchain. This problem should not occur as the routines to prepare the stack would fail themselves, but you'll never know. mkDir : Create a directory, using the Lua lfs package. It really has the effect effect of mkdir -p , so it can create multiple levels from the given directory if needed.","title":"Routines"},{"location":"design_decisions/","text":"Design decisions Software stack modules The name of the stack, calcua , is not hard-coded but specified through a variable in the system definition so that a future version may be extended to support more sites. The software stack has two levels: Version of the software stack: calcua module. Hardware architecture However, when loading a module from the first level we try to automatically determine the most suitable module for the second level. A hardware architecture can have subarchitectures, see the discussion in the \"The generic calcua and clusterarch modules\" page . The final decision is to go for only two levels: a generic one and one specialised for the full architecture (including the accelerator) though the implementation is done such that this could be changed for a later version of the software stack. It will create a lot more files as a lot more packages will be installed multiple times, but it reduces complexity and the chance for error of installing a package at the wrong level. The latter implies that we will need a double tree of modules One follows very strictly the Lmod hierarchy rules. Only one directory at any level of this tree can be present in the MODULEPATH . This tree is called the infrastructure modules . The second tree contains the actual software. Multiple subdirectories from this tree can be in the MODULEPATH at the same time, e.g., the directory for modules compiled with generic options and directory with modules optimised for a specific node type. To ensure that module swapping works correctly in Lmod (with Lmod trying to find equivalent versions when changing the hardware architecture), it is important that each software package is installed at one and only one level in the architecture hierarchy. So one should install a package either only for the generic architecture, or for all relevant specific architectures, but not in a generic way and some specific architectures. The module calcua/system is a special version of the calcua module for software that fulfills two requirements: We want it to be available for any regular version of the calcua stack It is build using the SYSTEM toolchain (or installed in an equivalent way) This is basically the tree to install software from binaries or to create modules for manually installed software (e.g., MATLAB and MAPLE). The stack is managed through EasyBuild though. The code of the module tree is largely prepared to have an architecture hierarchy there too, but we may not use it initially. The tree could be used to, e.g., accomodate Gaussian and automatically offer the right version rather than have multiple versions with versionsuffix for the architecture. 3 modules are needed to configure EasyBuild (though they are implemented as one generic module): EasyBuild-infrastructure for the few modules that need to be installed in the infrastructure tree (if any as on LUMI where this approach is also used basically only the toolchains are installed with EasyBuild, and they only belong in the infrastructure tree tp be able to always load the correct target modules as needed by the HPE Cray Programming Environment). EasyBuild-production for installing software with EasyBuild in the central software stack EasyBuild-user for installing software with EasyBuild in the user's directories. We support a 3-level and a 2-level naming scheme. At the top level and the bottom level (most generic level) there is no difference between both schemes. Top level always uses 3-component names: OS-CPU-Accelerator. Generic level names do not include an accelerator. In the ClusterMod_SystemTable that defines which architectures have software stacks for which stack version, only generic CPU names or CPU-Accelerator pairs are supported Impact of omitting that restriction and supporting a module and software hierarchy Changes to the get_stack_top function needed. Needs changes to get_osarchs and get_osarchs_reverse . Directory structure Separate module roots for infrastructure modules that follow a strict Lmod hierarchy and for the easybuild-managed modules that are arranged by software stack and architecture. We also put software and modules build with EasyBuild in two fully separate trees in the installation root rather than always putting the software and its modules next to each other as is done in the default EasyBuild configuration. We have chosen this option to be able to use short names for software directories without reducing the readability of the names of the module file directories, and as such also to keep the size of shebang lines with full paths under control to not hit kernel-imposed limits. Configuration part of the tree At the top of the hierarchy, in the installation root, there is an etc subdirectory with currently only the SoftwareStack.lua configuration file which is created automatically by the software stack installation script and is used to point to a number of important files and (sub)directories: The LUA system definition file that should be used The repository with the LMOD configuration and generic modules (see below) The repository with the whole EasyBuild setup, including custom EasyBlocks and custom EasyConfigs. Configuration part of the tree InstallRoot \u2514\u2500 etc (Optional) repository part of the tree At the top of the hierarchy, in the installation root, we find the GitHub repositories with the module system and the EasyBuild configuration, the roots of the infrastructure module system and the EasyBuild module system, the software packages tree, and the EasyBuild repo of installed software. Though both repositories are needed for the stack to work, it is possible to have them elsewhere, which may be a good option for test stacks or for development where you may want the repositories in a place that works better with your remote software development tools. (Optional) repository part of the tree InstallRoot \u251c\u2500 UAntwerpen-modules #(1) \u2514\u2500 UAntwerpen-easybuild #(2) \u2514\u2500 easybuild \u251c\u2500 easyconfigs \u251c\u2500 easyblocks \u251c\u2500 Customisations to naming schemes etc \u2514\u2500 config #(3) Repository with LMOD configuration and generic modules EasyBuild setup Configuration files for some settings not done via environment Module part of the tree We distinguish between two types of system-wide installed software that is not using EasyBuild toolchains (except SYSTEM ): Software installed via EasyBuild (though with some tricks) That software appears in the EasyBuild-managed modules and EasyBuild repo also. The software is installed via a dummy version of the calcua software stack ( calcua/system ) with its own copy of EasyBuild. Software installed without using EasyBuild, with modules that are generated via dummy EasyConfig files in the EasyBuild hierarchy. Infrastructure modules: modules-infrastructure The hierarchy is build using the long names for the architecture string init-UAntwerpen-modules : Outside the hierarchy: Subdirectory for the module(s) that initialises the whole module setup. Loading the initialisation module enables loading of the next level, the software stack modules. stacks subdirectory contains the modules for the software stack, currently only calcua modules, but that leaves room for a different software stack, e.g., EESSI, later onn. Loading of a calcua module enables the next level, loading of an architecture module. arch subdirectory contains the architecture modules (or cluster modules). 2 levels of subdirectories before you reach the architecture modules: Name of the stack: calcua Version of the stack Modules named after the cluster could be easier to recognise for some users. However, it may also be tricky to implement. Instead we can also use cluster names as the version of the arch modules, and this is organised via .modulerc.lua files for each software stack in the arch module subdirectory. Loading an architecture module enables the next step in the hierarchy, loading the infrastructure modules and software modules. infrastructure subdirectory then contains the specific infrastructure modules, e.g., the EasyBuild configuration modules. 4 levels of subdirectories before you reach the infrastructure modules Name of the stack: calcua Version of the stack arch Long name string of the architecture (OS-CPU-Accelerator except for OS-x86_64). EasyBuild-managed modules: modules-easybuild Here we treat the system-wide installed software which is independent from any calcua software stack as a separate software stack without version Architecture-wise both are treated the same, though we expect that most if not all system-wide installed software will be installed in a generic architecture subdirectory. 2 levels before arriving at the actual infrastructure modules: Name-version of the software stack, e.g., calcua-2021b or system Architecture string Manually managed modules: modules-manual A space to put modules for the manually installed software in case we want to hand-code these modules rather then inject them elsewhere via EasyBuild. The precise structure will be determined when the need arrises. This leads to the following view on the modules tree: InstallRoot \u251c\u2500 modules-infrastructure #(1) \u2502 \u251c\u2500 init-UAntwerpen-modules #(2) \u2502 \u251c\u2500 stacks #(3) \u2502 \u2502 \u2514\u2500 calcua \u2502 \u2502 \u2514\u2500 2021b.lua #(4) \u2502 \u251c\u2500 arch #(5) \u2502 \u2502 \u2514\u2500 calcua \u2502 \u2502 \u251c\u2500 2021b \u2502 \u2502 \u2502 \u2514\u2500 arch \u2502 \u2502 \u2502 \u251c\u2500 redhat8-x86_64 \u2502 \u2502 \u2502 \u251c\u2500 redhat8-broadwell-noaccel \u2502 \u2502 \u2502 \u2514\u2500 redhat8-broadwell-quadro \u2502 \u2502 \u2514\u2500 system \u2502 \u2502 \u2514\u2500 arch \u2502 \u2502 \u251c\u2500 redhat8-x86_64 \u2502 \u2502 \u2514\u2500 redhat8-ivybridge-noaccel \u2502 \u2514\u2500 infrastructure #(6) \u2502 \u2514\u2500 calcua \u2502 \u2514\u2500 2021b \u2502 \u2514\u2500 arch \u2502 \u2514\u2500 redhat8-ivybridge-noaccel \u2502 \u251c\u2500 EasyBuild-production \u2502 \u251c\u2500 EasyBuild-infrastructure \u2502 \u2514\u2500 EasyBuild-user \u251c\u2500 modules-easybuild #(7) \u2502 \u251c\u2500 calcua-2021b \u2502 \u2502 \u251c\u2500 redhat8_x86_64 #(8) \u2502 \u2502 \u251c\u2500 redhat8-broadwell-noaccel \u2502 \u2502 \u2514\u2500 redhat8-broadwell-quadro \u2502 \u2514\u2500 system* #(9) \u2502 \u251c\u2500 redhat8-x86_64 #(10) \u2502 \u2514\u2500 redhat8-ivybridge-noaccel #(11) \u2514\u2500 modules-manual #(12)! Lmod hierarchy as the framework of the module system Subdirectory for the startup module First level: Software stack modules Symbolic link to a generic module! Second level: Architecture of the stack Third level: Infrastructure modules, e.g., EasyBuild configuration Modules generated with EasyBuild Directory for potential generic builds if performance does not matter Modules outside the regular software stacks No specific processor versions, e.g., Matlab Specific processor version, e.g., Gaussian Manually generated modules - OPTIONAL Software directory SW This directory follows the same layout as the one for the EasyBuild-installed software, with two differences: At the architecture level, the short architecture string is used to save space There is yet another pseudo-stack for manually installed software, called MNL This directory has no corresponding modules directory in the EasyBuild-managed directory as it is not managed at all by EasyBuild. Resulting structure of the software directory InstallRoot \u2514\u2500 SW \u251c\u2500 calcua-2021b \u2502 \u251c\u2500 RH8-x86_64 \u2502 \u251c\u2500 RH8-BRW-host \u2502 \u2514\u2500 RH8-BRW-NVGP61GL \u251c\u2500 system #(1) \u2502 \u251c\u2500 RH8-x86_64 \u2502 \u2514\u2500 RH8-IVB-host \u2514\u2500 MNL #(2) Sometimes relatively empty subdirs if EasyBuild only creates a module... Manually installed software Manangement subdirectory mgmt subdirectory for all (nearly) files that are somehow system-generated. Current subdirectories: ebfiles_repo : EasyBuild repository, structured the same way as the modules-easybuild subdirectory. ebfiles_repo_infrastructure : EasyBuild repository, structured the same way as the modules-infrastructrue/infrastructure subdirectory. lmod-cache : Placeholder for Lmod cache files. Maybe we should follow the approach of TCL Environment Modules 5 and have a separate cache file per module subdirectory, or does this become too much? We can also go for one corresponding to each software stack and cluster architecture combination which would result in a single file per ( software stack, architecture) combination. Resulting structure of the management directory InstallRoot \u2514\u2500 mgmt \u251c\u2500 ebfiles_repo \u2502 \u251c\u2500 calcua-2021b \u2502 \u2502 \u251c\u2500 redhat8-x86_64 \u2502 \u2502 \u2514\u2500 redhat8-broadwell-noaccel \u2502 \u2514\u2500 system #(1) \u2502 \u2514\u2500 redhat8-x86_64 #(2) \u251c\u2500 ebfiles_repo_infrastructure \u2502 \u251c\u2500 calcua-2021b \u2502 \u2502 \u251c\u2500 redhat8-x86_64 \u2502 \u2502 \u2514\u2500 redhat8-broadwell-noaccel \u2502 \u2514\u2500 system #(1) \u2502 \u2514\u2500 redhat8-x86_64 #(2) \u2514\u2500 lmod_cache Modules outside the regular software stacks No specific processor versions, e.g., Matlab Other subdirectories sources subdirectory to permanently store the sources. This directory is further organised in the EasyBuild way. Downloads for manually installed software can be added to it by putting it in the EasyBuild structure. In the future we may need to add an additional level to distinguish between EasyBuild and other build tools that we may use and that have a different structure for storing source files. Other subdirectories InstallRoot \u2514\u2500 sources","title":"Design decisions"},{"location":"design_decisions/#design-decisions","text":"","title":"Design decisions"},{"location":"design_decisions/#software-stack-modules","text":"The name of the stack, calcua , is not hard-coded but specified through a variable in the system definition so that a future version may be extended to support more sites. The software stack has two levels: Version of the software stack: calcua module. Hardware architecture However, when loading a module from the first level we try to automatically determine the most suitable module for the second level. A hardware architecture can have subarchitectures, see the discussion in the \"The generic calcua and clusterarch modules\" page . The final decision is to go for only two levels: a generic one and one specialised for the full architecture (including the accelerator) though the implementation is done such that this could be changed for a later version of the software stack. It will create a lot more files as a lot more packages will be installed multiple times, but it reduces complexity and the chance for error of installing a package at the wrong level. The latter implies that we will need a double tree of modules One follows very strictly the Lmod hierarchy rules. Only one directory at any level of this tree can be present in the MODULEPATH . This tree is called the infrastructure modules . The second tree contains the actual software. Multiple subdirectories from this tree can be in the MODULEPATH at the same time, e.g., the directory for modules compiled with generic options and directory with modules optimised for a specific node type. To ensure that module swapping works correctly in Lmod (with Lmod trying to find equivalent versions when changing the hardware architecture), it is important that each software package is installed at one and only one level in the architecture hierarchy. So one should install a package either only for the generic architecture, or for all relevant specific architectures, but not in a generic way and some specific architectures. The module calcua/system is a special version of the calcua module for software that fulfills two requirements: We want it to be available for any regular version of the calcua stack It is build using the SYSTEM toolchain (or installed in an equivalent way) This is basically the tree to install software from binaries or to create modules for manually installed software (e.g., MATLAB and MAPLE). The stack is managed through EasyBuild though. The code of the module tree is largely prepared to have an architecture hierarchy there too, but we may not use it initially. The tree could be used to, e.g., accomodate Gaussian and automatically offer the right version rather than have multiple versions with versionsuffix for the architecture. 3 modules are needed to configure EasyBuild (though they are implemented as one generic module): EasyBuild-infrastructure for the few modules that need to be installed in the infrastructure tree (if any as on LUMI where this approach is also used basically only the toolchains are installed with EasyBuild, and they only belong in the infrastructure tree tp be able to always load the correct target modules as needed by the HPE Cray Programming Environment). EasyBuild-production for installing software with EasyBuild in the central software stack EasyBuild-user for installing software with EasyBuild in the user's directories. We support a 3-level and a 2-level naming scheme. At the top level and the bottom level (most generic level) there is no difference between both schemes. Top level always uses 3-component names: OS-CPU-Accelerator. Generic level names do not include an accelerator. In the ClusterMod_SystemTable that defines which architectures have software stacks for which stack version, only generic CPU names or CPU-Accelerator pairs are supported Impact of omitting that restriction and supporting a module and software hierarchy Changes to the get_stack_top function needed. Needs changes to get_osarchs and get_osarchs_reverse .","title":"Software stack modules"},{"location":"design_decisions/#directory-structure","text":"Separate module roots for infrastructure modules that follow a strict Lmod hierarchy and for the easybuild-managed modules that are arranged by software stack and architecture. We also put software and modules build with EasyBuild in two fully separate trees in the installation root rather than always putting the software and its modules next to each other as is done in the default EasyBuild configuration. We have chosen this option to be able to use short names for software directories without reducing the readability of the names of the module file directories, and as such also to keep the size of shebang lines with full paths under control to not hit kernel-imposed limits.","title":"Directory structure"},{"location":"design_decisions/#configuration-part-of-the-tree","text":"At the top of the hierarchy, in the installation root, there is an etc subdirectory with currently only the SoftwareStack.lua configuration file which is created automatically by the software stack installation script and is used to point to a number of important files and (sub)directories: The LUA system definition file that should be used The repository with the LMOD configuration and generic modules (see below) The repository with the whole EasyBuild setup, including custom EasyBlocks and custom EasyConfigs. Configuration part of the tree InstallRoot \u2514\u2500 etc","title":"Configuration part of the tree"},{"location":"design_decisions/#optional-repository-part-of-the-tree","text":"At the top of the hierarchy, in the installation root, we find the GitHub repositories with the module system and the EasyBuild configuration, the roots of the infrastructure module system and the EasyBuild module system, the software packages tree, and the EasyBuild repo of installed software. Though both repositories are needed for the stack to work, it is possible to have them elsewhere, which may be a good option for test stacks or for development where you may want the repositories in a place that works better with your remote software development tools. (Optional) repository part of the tree InstallRoot \u251c\u2500 UAntwerpen-modules #(1) \u2514\u2500 UAntwerpen-easybuild #(2) \u2514\u2500 easybuild \u251c\u2500 easyconfigs \u251c\u2500 easyblocks \u251c\u2500 Customisations to naming schemes etc \u2514\u2500 config #(3) Repository with LMOD configuration and generic modules EasyBuild setup Configuration files for some settings not done via environment","title":"(Optional) repository part of the tree"},{"location":"design_decisions/#module-part-of-the-tree","text":"We distinguish between two types of system-wide installed software that is not using EasyBuild toolchains (except SYSTEM ): Software installed via EasyBuild (though with some tricks) That software appears in the EasyBuild-managed modules and EasyBuild repo also. The software is installed via a dummy version of the calcua software stack ( calcua/system ) with its own copy of EasyBuild. Software installed without using EasyBuild, with modules that are generated via dummy EasyConfig files in the EasyBuild hierarchy. Infrastructure modules: modules-infrastructure The hierarchy is build using the long names for the architecture string init-UAntwerpen-modules : Outside the hierarchy: Subdirectory for the module(s) that initialises the whole module setup. Loading the initialisation module enables loading of the next level, the software stack modules. stacks subdirectory contains the modules for the software stack, currently only calcua modules, but that leaves room for a different software stack, e.g., EESSI, later onn. Loading of a calcua module enables the next level, loading of an architecture module. arch subdirectory contains the architecture modules (or cluster modules). 2 levels of subdirectories before you reach the architecture modules: Name of the stack: calcua Version of the stack Modules named after the cluster could be easier to recognise for some users. However, it may also be tricky to implement. Instead we can also use cluster names as the version of the arch modules, and this is organised via .modulerc.lua files for each software stack in the arch module subdirectory. Loading an architecture module enables the next step in the hierarchy, loading the infrastructure modules and software modules. infrastructure subdirectory then contains the specific infrastructure modules, e.g., the EasyBuild configuration modules. 4 levels of subdirectories before you reach the infrastructure modules Name of the stack: calcua Version of the stack arch Long name string of the architecture (OS-CPU-Accelerator except for OS-x86_64). EasyBuild-managed modules: modules-easybuild Here we treat the system-wide installed software which is independent from any calcua software stack as a separate software stack without version Architecture-wise both are treated the same, though we expect that most if not all system-wide installed software will be installed in a generic architecture subdirectory. 2 levels before arriving at the actual infrastructure modules: Name-version of the software stack, e.g., calcua-2021b or system Architecture string Manually managed modules: modules-manual A space to put modules for the manually installed software in case we want to hand-code these modules rather then inject them elsewhere via EasyBuild. The precise structure will be determined when the need arrises. This leads to the following view on the modules tree: InstallRoot \u251c\u2500 modules-infrastructure #(1) \u2502 \u251c\u2500 init-UAntwerpen-modules #(2) \u2502 \u251c\u2500 stacks #(3) \u2502 \u2502 \u2514\u2500 calcua \u2502 \u2502 \u2514\u2500 2021b.lua #(4) \u2502 \u251c\u2500 arch #(5) \u2502 \u2502 \u2514\u2500 calcua \u2502 \u2502 \u251c\u2500 2021b \u2502 \u2502 \u2502 \u2514\u2500 arch \u2502 \u2502 \u2502 \u251c\u2500 redhat8-x86_64 \u2502 \u2502 \u2502 \u251c\u2500 redhat8-broadwell-noaccel \u2502 \u2502 \u2502 \u2514\u2500 redhat8-broadwell-quadro \u2502 \u2502 \u2514\u2500 system \u2502 \u2502 \u2514\u2500 arch \u2502 \u2502 \u251c\u2500 redhat8-x86_64 \u2502 \u2502 \u2514\u2500 redhat8-ivybridge-noaccel \u2502 \u2514\u2500 infrastructure #(6) \u2502 \u2514\u2500 calcua \u2502 \u2514\u2500 2021b \u2502 \u2514\u2500 arch \u2502 \u2514\u2500 redhat8-ivybridge-noaccel \u2502 \u251c\u2500 EasyBuild-production \u2502 \u251c\u2500 EasyBuild-infrastructure \u2502 \u2514\u2500 EasyBuild-user \u251c\u2500 modules-easybuild #(7) \u2502 \u251c\u2500 calcua-2021b \u2502 \u2502 \u251c\u2500 redhat8_x86_64 #(8) \u2502 \u2502 \u251c\u2500 redhat8-broadwell-noaccel \u2502 \u2502 \u2514\u2500 redhat8-broadwell-quadro \u2502 \u2514\u2500 system* #(9) \u2502 \u251c\u2500 redhat8-x86_64 #(10) \u2502 \u2514\u2500 redhat8-ivybridge-noaccel #(11) \u2514\u2500 modules-manual #(12)! Lmod hierarchy as the framework of the module system Subdirectory for the startup module First level: Software stack modules Symbolic link to a generic module! Second level: Architecture of the stack Third level: Infrastructure modules, e.g., EasyBuild configuration Modules generated with EasyBuild Directory for potential generic builds if performance does not matter Modules outside the regular software stacks No specific processor versions, e.g., Matlab Specific processor version, e.g., Gaussian Manually generated modules - OPTIONAL","title":"Module part of the tree"},{"location":"design_decisions/#software-directory-sw","text":"This directory follows the same layout as the one for the EasyBuild-installed software, with two differences: At the architecture level, the short architecture string is used to save space There is yet another pseudo-stack for manually installed software, called MNL This directory has no corresponding modules directory in the EasyBuild-managed directory as it is not managed at all by EasyBuild. Resulting structure of the software directory InstallRoot \u2514\u2500 SW \u251c\u2500 calcua-2021b \u2502 \u251c\u2500 RH8-x86_64 \u2502 \u251c\u2500 RH8-BRW-host \u2502 \u2514\u2500 RH8-BRW-NVGP61GL \u251c\u2500 system #(1) \u2502 \u251c\u2500 RH8-x86_64 \u2502 \u2514\u2500 RH8-IVB-host \u2514\u2500 MNL #(2) Sometimes relatively empty subdirs if EasyBuild only creates a module... Manually installed software","title":"Software directory SW"},{"location":"design_decisions/#manangement-subdirectory","text":"mgmt subdirectory for all (nearly) files that are somehow system-generated. Current subdirectories: ebfiles_repo : EasyBuild repository, structured the same way as the modules-easybuild subdirectory. ebfiles_repo_infrastructure : EasyBuild repository, structured the same way as the modules-infrastructrue/infrastructure subdirectory. lmod-cache : Placeholder for Lmod cache files. Maybe we should follow the approach of TCL Environment Modules 5 and have a separate cache file per module subdirectory, or does this become too much? We can also go for one corresponding to each software stack and cluster architecture combination which would result in a single file per ( software stack, architecture) combination. Resulting structure of the management directory InstallRoot \u2514\u2500 mgmt \u251c\u2500 ebfiles_repo \u2502 \u251c\u2500 calcua-2021b \u2502 \u2502 \u251c\u2500 redhat8-x86_64 \u2502 \u2502 \u2514\u2500 redhat8-broadwell-noaccel \u2502 \u2514\u2500 system #(1) \u2502 \u2514\u2500 redhat8-x86_64 #(2) \u251c\u2500 ebfiles_repo_infrastructure \u2502 \u251c\u2500 calcua-2021b \u2502 \u2502 \u251c\u2500 redhat8-x86_64 \u2502 \u2502 \u2514\u2500 redhat8-broadwell-noaccel \u2502 \u2514\u2500 system #(1) \u2502 \u2514\u2500 redhat8-x86_64 #(2) \u2514\u2500 lmod_cache Modules outside the regular software stacks No specific processor versions, e.g., Matlab","title":"Manangement subdirectory"},{"location":"design_decisions/#other-subdirectories","text":"sources subdirectory to permanently store the sources. This directory is further organised in the EasyBuild way. Downloads for manually installed software can be added to it by putting it in the EasyBuild structure. In the future we may need to add an additional level to distinguish between EasyBuild and other build tools that we may use and that have a different structure for storing source files. Other subdirectories InstallRoot \u2514\u2500 sources","title":"Other subdirectories"},{"location":"directory_structure/","text":"Directory structure of the repository and software stack The modules repository Installed as 'UAntwerpen-modules' in the software stack directory Subdirectories docs : The technical documentation for the setup etc : Place to store the message-of-the-day file and the tips list. generic-modules : Generic implementation of the software stack and EasyBuild configuration modules. CalcUA-init : Initialisation module for the software stack, removing some of that stuff from the system images. calcua : Software stack module for the CalcUA software stacks. clusterarch : Module to select the desired machine architecture for the software. EasyBuild-config : Generic configuration module for EasyBuild StyleModifiers : modules to change the display style of the module tree. LMOD : Module with the configuration files for LMOD. scripts : Various scripts to set up and maintain the software stack. scripts-dev : Various scripts used during the development of the this repository, to test concepts. The UAntwerpen-easybuild repository This is the repository containing our full EasyBuild setup with custom EasyBlocks and EasyConfigs, the hooks file, configuration files, etc.","title":"Directory structure of the repositories and software stack"},{"location":"directory_structure/#directory-structure-of-the-repository-and-software-stack","text":"","title":"Directory structure of the repository and software stack"},{"location":"directory_structure/#the-modules-repository","text":"Installed as 'UAntwerpen-modules' in the software stack directory Subdirectories docs : The technical documentation for the setup etc : Place to store the message-of-the-day file and the tips list. generic-modules : Generic implementation of the software stack and EasyBuild configuration modules. CalcUA-init : Initialisation module for the software stack, removing some of that stuff from the system images. calcua : Software stack module for the CalcUA software stacks. clusterarch : Module to select the desired machine architecture for the software. EasyBuild-config : Generic configuration module for EasyBuild StyleModifiers : modules to change the display style of the module tree. LMOD : Module with the configuration files for LMOD. scripts : Various scripts to set up and maintain the software stack. scripts-dev : Various scripts used during the development of the this repository, to test concepts.","title":"The modules repository"},{"location":"directory_structure/#the-uantwerpen-easybuild-repository","text":"This is the repository containing our full EasyBuild setup with custom EasyBlocks and EasyConfigs, the hooks file, configuration files, etc.","title":"The UAntwerpen-easybuild repository"},{"location":"easybuild/","text":"The EasyBuild configuration Location of the EasyBuild modules Ideally we would have an EasyBuild module for each software stack stored in the generic architecture(s) for the software stack. However, we also need one for the system partition and it may not be a good idea to stick to an old version there. There are two options: One in the generic architecture(s) for calcua/system and one in each the generic architecture(s) for each of the calcua/yyyyx software stacks. This would guarantee that at least for the non-system stacks the EasyBuild version would remain constant, and it could even remain constant for the system stack for quite a while All EasyBuild installations are done in the generic architecture(s) of calcua/system . This would allow to use a different version of EasyBuild than intended for the stack. We did chose to go for the second option. However, we do let the EasyBuild config modules force-load the correct version so the user has to overwrite explicitly after loading that module. The stack does implement a script that will bootstrap EasyBuild rather than install EasyBuild with another version of EasyBuild. This makes it very easy to set up a test stack in a different file system. Configuration modes (modules) EasyBuild-production is the configuration for installing software with modules in the modules-easybuild tree. Where software will be installed, is determined by the software stack and cluster architecture modules that are loaded at that time. EasyBuild-user is the configuration for installing software in the user directory. The place where software will be installed in the user tree again depends on the software stack and cluster architecture modules that are loaded at that time. EasyBuild-infrastructure is the configuration for installing software whose modules go in the modules-infrastrcutrue/infrastructure tree. It is implemented but not really tested as we have no use for it at the moment. Environment variables EBU_USER_PREFIX : Directory for the user installation. User installation can be explicitly disabled by setting EBU_USER_PREFIX but not giving it a value. EBU_EASYBUILD_VERSIONLESS : When set and not 0 or no , the module will not load a specific version of EasyBuild. As such it would preserve whatever version a user has loaded already, or load the default version as determined by Lmod rules if no module is loaded. EBU_REMOTE_BUILD : When set and not 0 or no , choose a build and temporary directory that works everywhere so that EasyBuild can start Slurm jobs to build an application. Settings in configuration files Module syntax and naming scheme. Given that the whole module framework is based on Lmod, it is not a good idea though to use Tcl-based modules even though Lmod can work with them. Module naming scheme Modules that should be hidden. This is not currently used in the configuration at UAntwerpen. It may be better to hide modules through Lmod instead as then modules maintain their regular version numbers and it is easier to change visibility afterwards. Modules that may be loaded when EasyBuild runs. Setting to ignore EBROOT variables without matching modules as this is very useful to define additional EBROOT variables in bundles. Settings through environment variables Module installation path production : Determined by the get_system_module_dir function and based on the system installation root. infrastructure : Determined by the get_system_inframodule_dir function and based on the system installation root. user : Determined by the get_user_module_dir function and based on the user installation root. Software installation path production and infrastructure use the same path, determined by the get_system_SW_dir function and based on top of the system installation directory. user : Determined by the gget_user_SW_dir function and based on top of the user installation root. Installed EasyConfigs repository production and infrastructure use the same directory, determined by the get_system_EBrepo_dir function and based on top of the system installation directory. As we should not have modules with the same name in the infrastructure and in the regular module tree, there does not seem to be a problem with this approach. user : Determined by the get_user_EBrepo_dir function and based on top of the user installation directory. Sources subdirectories: production and infrastructure use the same directory, determined by the get_system_EBsources_dir function and based on top of the system installation directory. user stores the sources in the directory determined by the get_user_EBsources_dir function based on top of the user installation directory, but also adds the directory used by production to the sources path so that sources that are already on the system don't need to be downloaded again. Robot search path. The goal of our settings is that we ensure as much as possible that for installed modules EasyBuild will find the EasyConfig used for that install. Therefore we put the repository of installed EasyConfigs first in the robot path, even though that is not commonly done. This does have a negative aspect also though: When re-installing a module after a change to the EasyConfig, we need to re-install each package that needs to be reinstalled by hand from the directory of the corresponding EasyConfig, or the old one in the repository will be used instead. We use the EASYBUILD_ROBOT_PATHS environment variable so that dependency resolution is not turned on by default and the -r option remains available for users to add additional directories to the front of the search path. After that comes the default name for the user repository, if present and if the user module is being loaded. Next comes the system repository as can be obtained from etc/SystemDefinition.lua . Next we add the default EasyBuilders repository. EasyConfig search path for -S and --search : No additional directories are used at the moment. This can be changed should we chose to pull in additional repositories. EasyBlocks: In user mode, if there is a user repository and if it contains an easyblocks subdirectory, then that one has the highest priority (but looks like it has to go at the end of the list). In all modes, the next highest priority are the EasyBlocks in the easyblocks subdirectory in the system EasyBuild repository. Last in line are the EasyBlocks from the EasyBuild distribution that is being used. Configuration files: These are in the easybuild/config subdirectory of the system repo and the similar subdirectory in UserRepo of the user installation. In the order in which they should be read, so the later one wins on the earlier ones: In all modes, easybuild-production.cfg In user mode only, easybuild-user.cfg In all modes, easybuild-production-<stackname>-<stackversion>.cfg where the stack name for the system version is also the regular stack name. In user mode, easybuild-user-<stackname>-<stackversion>.cfg Build directory and temporary directory for EasyBuild: The subdirectories build and tmp respectively of the work directory determined according to the followin algorithm: If EBU_REMOTE_BUILD is set and nonzero (or not no ), then the work directory is /dev/shm/$USER/easybuild as that directory is available everywhere on the cluster and suitable for fast building. DISADVANTAGE AND REASON TO RECONSIDER: Currently this directory is not automatically cleaned when a job fails. If XDG_RUNTIME_DIR is defined, which is the case on the login nodes, then the subdirectory 'easybuild' of that directory is used. This directory is always automatically cleaned when the last session of a user terminates, but it does eat from RAM disk space also. Otherwise, if the environment variable SLURM_JOB_ID exits, use /dev/shm/$USER-$JOBID/easybuild . In that way a user can have multiple jobs on a node from which EasyBuild is used to build software, and these session can be using the same sourdces, e.g., to compile an application in different configurations. Otherwise we simply use /dev/shm/$USER/easybuild .","title":"EasyBuild configuration"},{"location":"easybuild/#the-easybuild-configuration","text":"","title":"The EasyBuild configuration"},{"location":"easybuild/#location-of-the-easybuild-modules","text":"Ideally we would have an EasyBuild module for each software stack stored in the generic architecture(s) for the software stack. However, we also need one for the system partition and it may not be a good idea to stick to an old version there. There are two options: One in the generic architecture(s) for calcua/system and one in each the generic architecture(s) for each of the calcua/yyyyx software stacks. This would guarantee that at least for the non-system stacks the EasyBuild version would remain constant, and it could even remain constant for the system stack for quite a while All EasyBuild installations are done in the generic architecture(s) of calcua/system . This would allow to use a different version of EasyBuild than intended for the stack. We did chose to go for the second option. However, we do let the EasyBuild config modules force-load the correct version so the user has to overwrite explicitly after loading that module. The stack does implement a script that will bootstrap EasyBuild rather than install EasyBuild with another version of EasyBuild. This makes it very easy to set up a test stack in a different file system.","title":"Location of the EasyBuild modules"},{"location":"easybuild/#configuration-modes-modules","text":"EasyBuild-production is the configuration for installing software with modules in the modules-easybuild tree. Where software will be installed, is determined by the software stack and cluster architecture modules that are loaded at that time. EasyBuild-user is the configuration for installing software in the user directory. The place where software will be installed in the user tree again depends on the software stack and cluster architecture modules that are loaded at that time. EasyBuild-infrastructure is the configuration for installing software whose modules go in the modules-infrastrcutrue/infrastructure tree. It is implemented but not really tested as we have no use for it at the moment.","title":"Configuration modes (modules)"},{"location":"easybuild/#environment-variables","text":"EBU_USER_PREFIX : Directory for the user installation. User installation can be explicitly disabled by setting EBU_USER_PREFIX but not giving it a value. EBU_EASYBUILD_VERSIONLESS : When set and not 0 or no , the module will not load a specific version of EasyBuild. As such it would preserve whatever version a user has loaded already, or load the default version as determined by Lmod rules if no module is loaded. EBU_REMOTE_BUILD : When set and not 0 or no , choose a build and temporary directory that works everywhere so that EasyBuild can start Slurm jobs to build an application.","title":"Environment variables"},{"location":"easybuild/#settings-in-configuration-files","text":"Module syntax and naming scheme. Given that the whole module framework is based on Lmod, it is not a good idea though to use Tcl-based modules even though Lmod can work with them. Module naming scheme Modules that should be hidden. This is not currently used in the configuration at UAntwerpen. It may be better to hide modules through Lmod instead as then modules maintain their regular version numbers and it is easier to change visibility afterwards. Modules that may be loaded when EasyBuild runs. Setting to ignore EBROOT variables without matching modules as this is very useful to define additional EBROOT variables in bundles.","title":"Settings in configuration files"},{"location":"easybuild/#settings-through-environment-variables","text":"Module installation path production : Determined by the get_system_module_dir function and based on the system installation root. infrastructure : Determined by the get_system_inframodule_dir function and based on the system installation root. user : Determined by the get_user_module_dir function and based on the user installation root. Software installation path production and infrastructure use the same path, determined by the get_system_SW_dir function and based on top of the system installation directory. user : Determined by the gget_user_SW_dir function and based on top of the user installation root. Installed EasyConfigs repository production and infrastructure use the same directory, determined by the get_system_EBrepo_dir function and based on top of the system installation directory. As we should not have modules with the same name in the infrastructure and in the regular module tree, there does not seem to be a problem with this approach. user : Determined by the get_user_EBrepo_dir function and based on top of the user installation directory. Sources subdirectories: production and infrastructure use the same directory, determined by the get_system_EBsources_dir function and based on top of the system installation directory. user stores the sources in the directory determined by the get_user_EBsources_dir function based on top of the user installation directory, but also adds the directory used by production to the sources path so that sources that are already on the system don't need to be downloaded again. Robot search path. The goal of our settings is that we ensure as much as possible that for installed modules EasyBuild will find the EasyConfig used for that install. Therefore we put the repository of installed EasyConfigs first in the robot path, even though that is not commonly done. This does have a negative aspect also though: When re-installing a module after a change to the EasyConfig, we need to re-install each package that needs to be reinstalled by hand from the directory of the corresponding EasyConfig, or the old one in the repository will be used instead. We use the EASYBUILD_ROBOT_PATHS environment variable so that dependency resolution is not turned on by default and the -r option remains available for users to add additional directories to the front of the search path. After that comes the default name for the user repository, if present and if the user module is being loaded. Next comes the system repository as can be obtained from etc/SystemDefinition.lua . Next we add the default EasyBuilders repository. EasyConfig search path for -S and --search : No additional directories are used at the moment. This can be changed should we chose to pull in additional repositories. EasyBlocks: In user mode, if there is a user repository and if it contains an easyblocks subdirectory, then that one has the highest priority (but looks like it has to go at the end of the list). In all modes, the next highest priority are the EasyBlocks in the easyblocks subdirectory in the system EasyBuild repository. Last in line are the EasyBlocks from the EasyBuild distribution that is being used. Configuration files: These are in the easybuild/config subdirectory of the system repo and the similar subdirectory in UserRepo of the user installation. In the order in which they should be read, so the later one wins on the earlier ones: In all modes, easybuild-production.cfg In user mode only, easybuild-user.cfg In all modes, easybuild-production-<stackname>-<stackversion>.cfg where the stack name for the system version is also the regular stack name. In user mode, easybuild-user-<stackname>-<stackversion>.cfg Build directory and temporary directory for EasyBuild: The subdirectories build and tmp respectively of the work directory determined according to the followin algorithm: If EBU_REMOTE_BUILD is set and nonzero (or not no ), then the work directory is /dev/shm/$USER/easybuild as that directory is available everywhere on the cluster and suitable for fast building. DISADVANTAGE AND REASON TO RECONSIDER: Currently this directory is not automatically cleaned when a job fails. If XDG_RUNTIME_DIR is defined, which is the case on the login nodes, then the subdirectory 'easybuild' of that directory is used. This directory is always automatically cleaned when the last session of a user terminates, but it does eat from RAM disk space also. Otherwise, if the environment variable SLURM_JOB_ID exits, use /dev/shm/$USER-$JOBID/easybuild . In that way a user can have multiple jobs on a node from which EasyBuild is used to build software, and these session can be using the same sourdces, e.g., to compile an application in different configurations. Otherwise we simply use /dev/shm/$USER/easybuild .","title":"Settings through environment variables"},{"location":"environment_variables/","text":"Environment variables Set by ClusterMod-init _<PREFIX>_INIT_FIRST_LOAD : Set to one as soon as the module has been loaded for the first time and used to avoid re-displaying the message-of-the-day when the module is reloaded because of a module purge . Set by the stack generic module <PREFIX>_STACK_NAME : Name of the software stack that is loaded. <PREFIX>_STACK_VERSION : version of the software stack that is loaded. <PREFIX>_STACK_NAME_VERSION : Name and version (format name/version) of the software stack that is loaded. Set by the clusterarch generic module <PREFIX>_ARCH_OSARCH : The long os + architecture string for the activated architecture. Set by LMOD <PREFIX>_FAMILY_CLUSTERMOD_SOFTWARESTACK <PREFIX>_FAMILY_CLUSTERMOD_SOFTWARESTACK_VERSION <PREFIX>_FAMILY_CLUSTERMOD_CLUSTERARCH <PREFIX>_FAMILY_CLUSTERMOD_CLUSTERARCH_VERSION","title":"Environment variables"},{"location":"environment_variables/#environment-variables","text":"","title":"Environment variables"},{"location":"environment_variables/#set-by-clustermod-init","text":"_<PREFIX>_INIT_FIRST_LOAD : Set to one as soon as the module has been loaded for the first time and used to avoid re-displaying the message-of-the-day when the module is reloaded because of a module purge .","title":"Set by ClusterMod-init"},{"location":"environment_variables/#set-by-the-stack-generic-module","text":"<PREFIX>_STACK_NAME : Name of the software stack that is loaded. <PREFIX>_STACK_VERSION : version of the software stack that is loaded. <PREFIX>_STACK_NAME_VERSION : Name and version (format name/version) of the software stack that is loaded.","title":"Set by the stack generic module"},{"location":"environment_variables/#set-by-the-clusterarch-generic-module","text":"<PREFIX>_ARCH_OSARCH : The long os + architecture string for the activated architecture.","title":"Set by the clusterarch generic module"},{"location":"environment_variables/#set-by-lmod","text":"<PREFIX>_FAMILY_CLUSTERMOD_SOFTWARESTACK <PREFIX>_FAMILY_CLUSTERMOD_SOFTWARESTACK_VERSION <PREFIX>_FAMILY_CLUSTERMOD_CLUSTERARCH <PREFIX>_FAMILY_CLUSTERMOD_CLUSTERARCH_VERSION","title":"Set by LMOD"},{"location":"modules_calcua-init/","text":"The CalcUA-init module The CalcUA-init module is used to partially initialise the CalcUA software stack, moving some of that work out of the images on the compute nodes and into the software stack framework. It functions can include: Printing the variable part of the message-of-the-day Providing an interesting tip to a user, similar to the way fortune worked on old UNIX machines. Loading the initial environment Current implementation Build the MODULEPATH: Add style modifier modules Add the software stacks Set the module display style by loading ModuleColour , ModuleExtensions and ModuleLabel except when those modules are already loaded, and make sure they are not unloaded when the initialisation module is unloaded. That way a user can easily overwrite the display style by loading modules. For now: We do have a provision to show the message of the day and/or some tips.","title":"The CalcUA-init module"},{"location":"modules_calcua-init/#the-calcua-init-module","text":"The CalcUA-init module is used to partially initialise the CalcUA software stack, moving some of that work out of the images on the compute nodes and into the software stack framework. It functions can include: Printing the variable part of the message-of-the-day Providing an interesting tip to a user, similar to the way fortune worked on old UNIX machines. Loading the initial environment","title":"The CalcUA-init module"},{"location":"modules_calcua-init/#current-implementation","text":"Build the MODULEPATH: Add style modifier modules Add the software stacks Set the module display style by loading ModuleColour , ModuleExtensions and ModuleLabel except when those modules are already loaded, and make sure they are not unloaded when the initialisation module is unloaded. That way a user can easily overwrite the display style by loading modules. For now: We do have a provision to show the message of the day and/or some tips.","title":"Current implementation"},{"location":"modules_calcua_and_clusterarch/","text":"The generic calcua and clusterarch modules The calcua and clusterarch modules work together to load a version of the software stack for the right architecture. Moreover they ensure that the EasyBuild-config module can recover all the information it needs to install software in the right directories. The calcua module is loaded first. This module sets the version of the software stack. The version of the software stack is of the form year followed by the letter a or b. TODO : Think about support for yyyy.mm also as this is used during the development of new toolchains in EasyBuild itself, and also for EESSI. The calcua modules will automatically load the best fitting clusterarch module for the node on which the module is loaded. However, it is always possible to overwrite that option later on. Instances of the generic clusterarch module come in two shapes: arch/<target> or arch/<OS>-<target> : This is a direct mapping to the underlying directory structure. cluster/<name> or cluster/<name>-<OS> or cluster/<name>-<target> or cluster/<name>-<OS>-<target> or any other special designator: This is because users will typically be more familiar with the name of the cluster than with the CPU architecture. The mapping to CPU architecture is coded internally. We currently implement those through module name aliases. The and fields correspond to what would be used in Spack, with the exception that we use skylake instead of skylake_avx512 . The calcua generic module What the module does: Declare itself a sticky module from the family ClusterMod_SoftwareStack . Determine its name and version of the software stack from the module name and version Determine the architecture of the node on which it is executing taking into account the architectures supported by the current version of the calcua stack. Determine the architecture of the node Then look for the best match for the software stack that is being loaded. Enable loading of the architecture modules supported by the specific version of the software stack. Load the most suitable clusterarch module The clusterarch generic module In the current implementation we only support architecture strings as the version of the module. Instead we use a .modulerc.lua file in the the arch module subdirectory for each software stack to define synonyms based on the cluster names. This file is prepared as one of the steps in the prepare_calcua_stack.lua script. What the module does: Declare itself a sticky module from the family ClusterMod_clusterarch . Check location for the user stack via EBU_USER_PREFIX or the default, turn off by setting EBU_USER_PREFIX to an empty value. The default location is $VSC_DATA/EasyBuild REMAiNDER YET TO BE IMPLEMENTED! Determine suitable architecture strings: Highest-level architecture string for the current version of the software stack is determined from the version of the module. When we are not loading for the system version of the software stack, we also need to determine the highest level version of that stack that should be loaded. Determine the list of directories of EasyBuild-generated module files to load. Keep in mind that later on we will have to synchronise that with a list of directories in the EasyBuild installation repository. All user modules take a higher priority than any EasyBuild-generated module. Determine the infrastructure module directory to load. There are no user versions of infrastructure modules. FUTURE EXTENSION: Determine the manually generated modules directory/is to load. We may provide a placeholder for user-build manual modules also Generate prepend_path statements in reverse order So first for the system modules, in the order Manual (when implemented) Infrastructure system dummy stack from generic to specific Stack version from generic to specific Then for the user modules Manual (when implemented) system dummy stack from generic to specific Stack version from generic to specific Naming the cluster architecture (clusterarch) Preliminary notes The lists below are very complete lists, and certainly at the moment we do no have a separate software stack for them and some clusterarchs will be mapped on others (e.g., no specific stacks on the GPU nodes) but this may be revised over time. Depending on where they are used, there are long and short names. Each name consists of three components: Operating system + version CPU architecture (Optional) the accelerator, where we also have a term for no accelerator. The thre components are spearated by dashes. Note for the long names: The CPU target is based on the target that spack would report (or for that reason the archspec tool), except that we use skylake instead of skylake_avx512 and don't distinguish with Cascade Lake as that is just a different stepping fo the same CPU family and model. The OS name is also based on what spack would report In doing so, we hopefully ensure that the stack is future-proof and will use similar naming as what would be used in Spack or in EESSI, as that also follows these conventions for CPU targets. For the accelerators we've chosen user-friendly names rather than their technical names. Note for the short names: For the AMD processors we simply refer to the zen name as that is already a very short name. For the Intel processors we use established abbreviations: IVB for Ivy Bridge, BRW for Broadwell and SKLX for Skylake (in fact, the more often used abbreviation for Skylake with AVX512 is SKL-X but we want to avoid dashes in the name). For the AMD GPUs we refer to their internal GFX code which is often used in architecture strings for compilers with OpenMP offload. For NVIDIA GPUs we refer to their Compute Capability as that is a parameter that needs to be set when compiling. Furthermore we add \"GL\" to the string if it is a GPU for OpenGL visualisation. Hence: The Ampere A100 becomes NVCC80 as it has compute capability 8.0. The Tesla P100 becomes NVCC60 as it has compute capability 6.0. The Quadro P5000 which is based on the GP104 chip becomes NVCC61GL as it has compute capability 6.1 and is also meant for visualisation with OpenGL. OS names Currently supported for the OS component of the name: long short What? redhat7 RH7 Red Hat 7-compatible OS redhat8 RH8 Red Hat 8-compatible OS These names are defined in SitePackage_arch_hierarchy.lua . CPU names Currently supported for the CPU component of the name: long short What? ivybridge IVB Intel Ivy Bridge generation (E5-XXXX v2) broadwell BRW Intel Broadwell generation (E5-XXXX v4) skylake SKLX Intel Skylake and Cascadelake server CPUs zen2 zen2 AMD Zen2 generation x86_64 x86_64 Generic x86 64-bit CPU These names are defined in SitePackage_arch_hierarchy.lua . Accelerator names Here the short name isn't always shorter, but tells more about the architecture. at least for the GPU accelerators. long short What? noaccel host No accelerator in the node ampere NVCC80 NVIDIA A100 pascal NVCC60 NVIDIA P100 P5000 NVGP61GL NVIDIA P5000 visualisation node arcturus GFX908 AMD MI100 aurora1 NEC1 NEC Aurora 1st gen vector board These names are defined in SitePackage_arch_hierarchy.lua . Cluster architecture strings These combinations that are supported for each software stack version are defined in etc/SystemDefinition.lua . long short redhat8-x86_64 RH8-x86_64 redhat8-zen2 RH8-zen2 redhat8-zen2-noaccel RH8-zen2-host redhat8-zen2-ampere RH8-zen2-NVCC80 redhat8-zen2-arcturus RH8-zen2-GFX908 redhat8-broadwell RH8-BRW redhat8-broadwell-noaccel RH8-BRW-host redhat8-broadwell-P5000 RH8-BRW-NVGP61GL redhat8-broadwell-pascal RH8-BRW-NVCC60 redhat8-ivybridge RH8-IVB redhat8-skylake RH8-SKLX redhat8-skylake-noaccel RH8-SKLX-host redhat8-skylake-aurora1 RH8-SKLX-NEC1 Possible names for arch (or alternative cluster) modules based on the cluster name arch/hopper = arch/redhat8-ivybridge arch/leibniz = arch/redhat8-broadwell or arch/redhat8-broadwell-noaccel (depending on choices discussed further down) arch/leibniz-viz = arch/redhat8-broadwell-P5000 arch/leibniz-nvidia = arch/redhat8-broadwell-pascal arch/vaughan = arch/redhat8-zen2 or arch/cents8-rome-noaccel arch/vaughn-amd = arch/redhat8-zen2-arcturus arch/vaughan-nvidia = arch/redhat8-zen2-ampere arch/biomina = arch/redhat8-skylake or arch/redhat8-skylake-noaccel depending on choices discussed further down arch/aurora = arch/redhat8-skylake-aurora1 There is a mapping per software stack version defined in etc/SystemDefinition.lua . Determining the architecture string There are three options to determine the architecture string: Based on the name of the node Based on VSC_ environment variables Determine by reading some OS files and pseudo-files Based on names It is difficult to get hostname and domain in one. hostname -f does not have the desired effect on vaughan. One workaround is to use host $( hostname -i ) | awk '{print $NF }' Results: Node type host $(hostname -i) \\| awk '{print $NF }' login node vaughan lnX.vaughan.antwerpen.vsc compute node vaughan rXcYYcnZ.vaughan.antwerpen.vsc NVIDIA node vaughan nvam1.vaughan.antwerpen.vsc MI100 node vaughan amdarcX.vaughan.antwerpen.vsc login node leibniz lnX.leibniz.antwerpen.vsc visualisation node leibniz vizX.leibniz.antwerpen.vsc compute node leibniz rXcYYcnZ.leibniz.antwerpen.vsc compute node hopper rXcYYcnZ.hopper.antwerpen.vsc Pascal node leibniz paX.leibniz.antwerpen.vsc Aurora node leibniz aurora.leibniz.antwerpen.vsc Biomina node leibniz r0c03cZ.leibniz.antwerpen.vsc TODO: Is this still valid after the upgrade? No! Based on VSC_ variables Variables: VSC_ARCH_LOCAL: Currently ivybridge, broadwell or rome. The BioMina node also returns VSC_ARCH_LOCAL=broadwell... What about the aurora node? VSC_OS_LOCAL: centos7 or redhat8 It is not possible to detect the accelerator type Reading information in /proc etc. Get the CPU type: cat /proc/cpuinfo | grep -m 1 \"model name\" | cut -d : -f 2 The only problem with this one is that it will still have a leading space but that is easily dealt with in the subsequent pattern matching to find the CPU family. Instead of extracting the \"model name\" line one could also extract the \"vendor_id\", \"cpu family\" and \"model\" lines. E.g., AMD Rome: CPU family 23, model 49. Intel Ivy Bridge: CPU family 6, model 62 Intel Broadwell: CPU family 6, model 79 Intel Skylake: CPU family 6, model 85 Intel Cascade Lake: Actually also CPU family 6, model 85, just a different stepping: 7 versus 4 for our Skylake CPUs Accelerators can be detected by looking in the output of the lspci command The OS can be detected from the variables that can be set through /etc/os-release, and in particular the NAME and VERSION_ID lines. Node type vendor_id cpu family model lspci login node vaughan AuthenticAMD 23 49 / Rome compute node vaughan AuthenticAMD 23 49 / NVIDIA node vaughan AuthenticAMD 23 49 / MI100 node vaughan AuthenticAMD 23 49 MI100 login node leibniz GenuineIntel 6 79 GA100 visualisation node leibniz GenuineIntel 6 79 GP104GL compute node leibniz GenuineIntel 6 79 / compute node hopper GenuineIntel 6 62 / Pascal node leibniz GenuineIntel 6 79 GP100GL Aurora node leibniz GenuineIntel 6 85 NEC Biomina node leibniz GenuineIntel 6 85 / Final solution The solution chosen was the last one, reading information from /proc/cpuinfo , /etc/os-release and the output of lspci . The detection is implemented in LMOD/SitePackage_system_info.lua . Binary versions loaded by the cluster and arch modules It is possible to implement the generic CalcUA module in such a way that one can change to a different scheme for a different toolchain version, but one should take into account that this might lead to confusion for users. However, from a user's perspective, option 1 and option 2 with the three-component names are not that different as they see the same architectures. Option 1: Maximal common installations There are almost always three levels: Level 1: Unoptimized generic x86 64-bit CPU Level 2: Specific CPU architecture, but the package is fully GPU agnostic Level 3: Specific CPU architecture and the package may have accelerated versions that we need to install with the same name. Combinations: Node L1 (generic) L2 L3 login/compute vaughan redhat8-x86_64 redhat8-zen2 redhat8-zen2-noaccel vaughan Ampere node redhat8-x86_64 redhat8-zen2 redhat8-zen2-ampere vaughan MI100 redhat8-x86_64 redhat8-zen2 redhat8-zen2-arcturus login/compute leibniz redhat8-x86_64 redhat8-broadwell redhat8-broadwell-noaccel leibniz visualisation redhat8-x86_64 redhat8-broadwell redhat8-broadwell-P5000 leibniz Pascal redhat8-x86_64 redhat8-broadwell redhat8-broadwell-pascal BioMina node redhat8-x86_64 redhat8-skylake redhat8-skylake-noaccel Leibniz aurora redhat8-x86_64 redhat8-skylake redhat8-skylake-aurora1 Hopper node redhat8-x86_64 redhat8-ivybridge / For Hopper we could drop the highest level as we only have nodes without accelerator and no new nodes with this CPU will ever come in. Advantages: Minimal number of duplicated installations Disadvantages: Very easy to make mistakes about what to install where. No package should be installed with the same full name (name + version + versionsuffix) at multiple levels. No package can have dependencies at a higher level. As it would be dangerous to have both an OpenMPI with and without accelerator support loaded, it means we have to install OpenMPI and all packages that depend on it at level 3, so we need to be very careful here. Everything installed with Intel that does not need a GPU can be installed at level 2 as there is no GPU-specific Intel MPI. With FOSS and its subtoolchains the situation is different. GCCcore and GCC can be installed at level 1 or 2 but as long as we do not know if the EasyBuild community will succeed at building a single MPI module that works for everything, gompi and foss software should be installed at level 3. Option 2: Only common installations for software such as Matlab or maybe system toolchain Now there are two levels: Level 1: Unoptimized generic x86 64-bit CPU Level 2: Optimised software, with 2 options for naming: use the shortest name possible or always use three components in the name: OS, CPU and accelerator. We will not implement the first option for Level 2 as this would only create confusion if we would ever want to mix schemes and switch to the first option for a new version of the software stack. Node L1 (generic) L2 (shortest - not implemented) L2 (3-component) login/compute vaughan redhat8-x86_64 redhat8-zen2 redhat8-zen2-noaccel vaughan Ampere node redhat8-x86_64 redhat8-zen2-ampere redhat8-zen2-ampere vaughan MI100 redhat8-x86_64 redhat8-zen2-arcturus redhat8-zen2-arcturus login/compute leibniz redhat8-x86_64 redhat8-broadwell redhat8-broadwell-noaccel leibniz visualisation redhat8-x86_64 redhat8-broadwell-P5000 redhat8-broadwell-P5000 leibniz Pascal redhat8-x86_64 redhat8-broadwell-pascal redhat8-broadwell-pascal BioMina node redhat8-x86_64 redhat8-skylake redhat8-skylake-noaccel Leibniz aurora redhat8-x86_64 redhat8-skylake-aurora1 redhat8-skylake-aurora1 Hopper node redhat8-x86_64 redhat8-ivybridge redhat8-ivybridge-noaccel Users could in principle still use software from another architecture within the stack by loading the appropriate clusterarch module so we could still be fairly selective about what we provide for the \"special\" nodes with accelerators. However, many of the often recurring dependencies like alternatives for what are often basic OS libraries would have to be installed multiple times. In this case the \"noaccel\" architecture isn't really needed unless we want all names to have three components if they are on level 2 (which may ease a transition to option 1 later on). Advantages Conceptually certainly simpler as there is little doubt about where to install a module Disadvantages Larger volume of the overall software stack as more modules will be duplicated.","title":"The generic calcua and clusterarch modules"},{"location":"modules_calcua_and_clusterarch/#the-generic-calcua-and-clusterarch-modules","text":"The calcua and clusterarch modules work together to load a version of the software stack for the right architecture. Moreover they ensure that the EasyBuild-config module can recover all the information it needs to install software in the right directories. The calcua module is loaded first. This module sets the version of the software stack. The version of the software stack is of the form year followed by the letter a or b. TODO : Think about support for yyyy.mm also as this is used during the development of new toolchains in EasyBuild itself, and also for EESSI. The calcua modules will automatically load the best fitting clusterarch module for the node on which the module is loaded. However, it is always possible to overwrite that option later on. Instances of the generic clusterarch module come in two shapes: arch/<target> or arch/<OS>-<target> : This is a direct mapping to the underlying directory structure. cluster/<name> or cluster/<name>-<OS> or cluster/<name>-<target> or cluster/<name>-<OS>-<target> or any other special designator: This is because users will typically be more familiar with the name of the cluster than with the CPU architecture. The mapping to CPU architecture is coded internally. We currently implement those through module name aliases. The and fields correspond to what would be used in Spack, with the exception that we use skylake instead of skylake_avx512 .","title":"The generic calcua and clusterarch modules"},{"location":"modules_calcua_and_clusterarch/#the-calcua-generic-module","text":"What the module does: Declare itself a sticky module from the family ClusterMod_SoftwareStack . Determine its name and version of the software stack from the module name and version Determine the architecture of the node on which it is executing taking into account the architectures supported by the current version of the calcua stack. Determine the architecture of the node Then look for the best match for the software stack that is being loaded. Enable loading of the architecture modules supported by the specific version of the software stack. Load the most suitable clusterarch module","title":"The calcua generic module"},{"location":"modules_calcua_and_clusterarch/#the-clusterarch-generic-module","text":"In the current implementation we only support architecture strings as the version of the module. Instead we use a .modulerc.lua file in the the arch module subdirectory for each software stack to define synonyms based on the cluster names. This file is prepared as one of the steps in the prepare_calcua_stack.lua script. What the module does: Declare itself a sticky module from the family ClusterMod_clusterarch . Check location for the user stack via EBU_USER_PREFIX or the default, turn off by setting EBU_USER_PREFIX to an empty value. The default location is $VSC_DATA/EasyBuild REMAiNDER YET TO BE IMPLEMENTED! Determine suitable architecture strings: Highest-level architecture string for the current version of the software stack is determined from the version of the module. When we are not loading for the system version of the software stack, we also need to determine the highest level version of that stack that should be loaded. Determine the list of directories of EasyBuild-generated module files to load. Keep in mind that later on we will have to synchronise that with a list of directories in the EasyBuild installation repository. All user modules take a higher priority than any EasyBuild-generated module. Determine the infrastructure module directory to load. There are no user versions of infrastructure modules. FUTURE EXTENSION: Determine the manually generated modules directory/is to load. We may provide a placeholder for user-build manual modules also Generate prepend_path statements in reverse order So first for the system modules, in the order Manual (when implemented) Infrastructure system dummy stack from generic to specific Stack version from generic to specific Then for the user modules Manual (when implemented) system dummy stack from generic to specific Stack version from generic to specific","title":"The clusterarch generic module"},{"location":"modules_calcua_and_clusterarch/#naming-the-cluster-architecture-clusterarch","text":"","title":"Naming the cluster architecture (clusterarch)"},{"location":"modules_calcua_and_clusterarch/#preliminary-notes","text":"The lists below are very complete lists, and certainly at the moment we do no have a separate software stack for them and some clusterarchs will be mapped on others (e.g., no specific stacks on the GPU nodes) but this may be revised over time. Depending on where they are used, there are long and short names. Each name consists of three components: Operating system + version CPU architecture (Optional) the accelerator, where we also have a term for no accelerator. The thre components are spearated by dashes. Note for the long names: The CPU target is based on the target that spack would report (or for that reason the archspec tool), except that we use skylake instead of skylake_avx512 and don't distinguish with Cascade Lake as that is just a different stepping fo the same CPU family and model. The OS name is also based on what spack would report In doing so, we hopefully ensure that the stack is future-proof and will use similar naming as what would be used in Spack or in EESSI, as that also follows these conventions for CPU targets. For the accelerators we've chosen user-friendly names rather than their technical names. Note for the short names: For the AMD processors we simply refer to the zen name as that is already a very short name. For the Intel processors we use established abbreviations: IVB for Ivy Bridge, BRW for Broadwell and SKLX for Skylake (in fact, the more often used abbreviation for Skylake with AVX512 is SKL-X but we want to avoid dashes in the name). For the AMD GPUs we refer to their internal GFX code which is often used in architecture strings for compilers with OpenMP offload. For NVIDIA GPUs we refer to their Compute Capability as that is a parameter that needs to be set when compiling. Furthermore we add \"GL\" to the string if it is a GPU for OpenGL visualisation. Hence: The Ampere A100 becomes NVCC80 as it has compute capability 8.0. The Tesla P100 becomes NVCC60 as it has compute capability 6.0. The Quadro P5000 which is based on the GP104 chip becomes NVCC61GL as it has compute capability 6.1 and is also meant for visualisation with OpenGL.","title":"Preliminary notes"},{"location":"modules_calcua_and_clusterarch/#os-names","text":"Currently supported for the OS component of the name: long short What? redhat7 RH7 Red Hat 7-compatible OS redhat8 RH8 Red Hat 8-compatible OS These names are defined in SitePackage_arch_hierarchy.lua .","title":"OS names"},{"location":"modules_calcua_and_clusterarch/#cpu-names","text":"Currently supported for the CPU component of the name: long short What? ivybridge IVB Intel Ivy Bridge generation (E5-XXXX v2) broadwell BRW Intel Broadwell generation (E5-XXXX v4) skylake SKLX Intel Skylake and Cascadelake server CPUs zen2 zen2 AMD Zen2 generation x86_64 x86_64 Generic x86 64-bit CPU These names are defined in SitePackage_arch_hierarchy.lua .","title":"CPU names"},{"location":"modules_calcua_and_clusterarch/#accelerator-names","text":"Here the short name isn't always shorter, but tells more about the architecture. at least for the GPU accelerators. long short What? noaccel host No accelerator in the node ampere NVCC80 NVIDIA A100 pascal NVCC60 NVIDIA P100 P5000 NVGP61GL NVIDIA P5000 visualisation node arcturus GFX908 AMD MI100 aurora1 NEC1 NEC Aurora 1st gen vector board These names are defined in SitePackage_arch_hierarchy.lua .","title":"Accelerator names"},{"location":"modules_calcua_and_clusterarch/#cluster-architecture-strings","text":"These combinations that are supported for each software stack version are defined in etc/SystemDefinition.lua . long short redhat8-x86_64 RH8-x86_64 redhat8-zen2 RH8-zen2 redhat8-zen2-noaccel RH8-zen2-host redhat8-zen2-ampere RH8-zen2-NVCC80 redhat8-zen2-arcturus RH8-zen2-GFX908 redhat8-broadwell RH8-BRW redhat8-broadwell-noaccel RH8-BRW-host redhat8-broadwell-P5000 RH8-BRW-NVGP61GL redhat8-broadwell-pascal RH8-BRW-NVCC60 redhat8-ivybridge RH8-IVB redhat8-skylake RH8-SKLX redhat8-skylake-noaccel RH8-SKLX-host redhat8-skylake-aurora1 RH8-SKLX-NEC1","title":"Cluster architecture strings"},{"location":"modules_calcua_and_clusterarch/#possible-names-for-arch-or-alternative-cluster-modules-based-on-the-cluster-name","text":"arch/hopper = arch/redhat8-ivybridge arch/leibniz = arch/redhat8-broadwell or arch/redhat8-broadwell-noaccel (depending on choices discussed further down) arch/leibniz-viz = arch/redhat8-broadwell-P5000 arch/leibniz-nvidia = arch/redhat8-broadwell-pascal arch/vaughan = arch/redhat8-zen2 or arch/cents8-rome-noaccel arch/vaughn-amd = arch/redhat8-zen2-arcturus arch/vaughan-nvidia = arch/redhat8-zen2-ampere arch/biomina = arch/redhat8-skylake or arch/redhat8-skylake-noaccel depending on choices discussed further down arch/aurora = arch/redhat8-skylake-aurora1 There is a mapping per software stack version defined in etc/SystemDefinition.lua .","title":"Possible names for arch (or alternative cluster) modules based on the cluster name"},{"location":"modules_calcua_and_clusterarch/#determining-the-architecture-string","text":"There are three options to determine the architecture string: Based on the name of the node Based on VSC_ environment variables Determine by reading some OS files and pseudo-files","title":"Determining the architecture string"},{"location":"modules_calcua_and_clusterarch/#based-on-names","text":"It is difficult to get hostname and domain in one. hostname -f does not have the desired effect on vaughan. One workaround is to use host $( hostname -i ) | awk '{print $NF }' Results: Node type host $(hostname -i) \\| awk '{print $NF }' login node vaughan lnX.vaughan.antwerpen.vsc compute node vaughan rXcYYcnZ.vaughan.antwerpen.vsc NVIDIA node vaughan nvam1.vaughan.antwerpen.vsc MI100 node vaughan amdarcX.vaughan.antwerpen.vsc login node leibniz lnX.leibniz.antwerpen.vsc visualisation node leibniz vizX.leibniz.antwerpen.vsc compute node leibniz rXcYYcnZ.leibniz.antwerpen.vsc compute node hopper rXcYYcnZ.hopper.antwerpen.vsc Pascal node leibniz paX.leibniz.antwerpen.vsc Aurora node leibniz aurora.leibniz.antwerpen.vsc Biomina node leibniz r0c03cZ.leibniz.antwerpen.vsc TODO: Is this still valid after the upgrade? No!","title":"Based on names"},{"location":"modules_calcua_and_clusterarch/#based-on-vsc_-variables","text":"Variables: VSC_ARCH_LOCAL: Currently ivybridge, broadwell or rome. The BioMina node also returns VSC_ARCH_LOCAL=broadwell... What about the aurora node? VSC_OS_LOCAL: centos7 or redhat8 It is not possible to detect the accelerator type","title":"Based on VSC_ variables"},{"location":"modules_calcua_and_clusterarch/#reading-information-in-proc-etc","text":"Get the CPU type: cat /proc/cpuinfo | grep -m 1 \"model name\" | cut -d : -f 2 The only problem with this one is that it will still have a leading space but that is easily dealt with in the subsequent pattern matching to find the CPU family. Instead of extracting the \"model name\" line one could also extract the \"vendor_id\", \"cpu family\" and \"model\" lines. E.g., AMD Rome: CPU family 23, model 49. Intel Ivy Bridge: CPU family 6, model 62 Intel Broadwell: CPU family 6, model 79 Intel Skylake: CPU family 6, model 85 Intel Cascade Lake: Actually also CPU family 6, model 85, just a different stepping: 7 versus 4 for our Skylake CPUs Accelerators can be detected by looking in the output of the lspci command The OS can be detected from the variables that can be set through /etc/os-release, and in particular the NAME and VERSION_ID lines. Node type vendor_id cpu family model lspci login node vaughan AuthenticAMD 23 49 / Rome compute node vaughan AuthenticAMD 23 49 / NVIDIA node vaughan AuthenticAMD 23 49 / MI100 node vaughan AuthenticAMD 23 49 MI100 login node leibniz GenuineIntel 6 79 GA100 visualisation node leibniz GenuineIntel 6 79 GP104GL compute node leibniz GenuineIntel 6 79 / compute node hopper GenuineIntel 6 62 / Pascal node leibniz GenuineIntel 6 79 GP100GL Aurora node leibniz GenuineIntel 6 85 NEC Biomina node leibniz GenuineIntel 6 85 /","title":"Reading information in /proc etc."},{"location":"modules_calcua_and_clusterarch/#final-solution","text":"The solution chosen was the last one, reading information from /proc/cpuinfo , /etc/os-release and the output of lspci . The detection is implemented in LMOD/SitePackage_system_info.lua .","title":"Final solution"},{"location":"modules_calcua_and_clusterarch/#binary-versions-loaded-by-the-cluster-and-arch-modules","text":"It is possible to implement the generic CalcUA module in such a way that one can change to a different scheme for a different toolchain version, but one should take into account that this might lead to confusion for users. However, from a user's perspective, option 1 and option 2 with the three-component names are not that different as they see the same architectures.","title":"Binary versions loaded by the cluster and arch modules"},{"location":"modules_calcua_and_clusterarch/#option-1-maximal-common-installations","text":"There are almost always three levels: Level 1: Unoptimized generic x86 64-bit CPU Level 2: Specific CPU architecture, but the package is fully GPU agnostic Level 3: Specific CPU architecture and the package may have accelerated versions that we need to install with the same name. Combinations: Node L1 (generic) L2 L3 login/compute vaughan redhat8-x86_64 redhat8-zen2 redhat8-zen2-noaccel vaughan Ampere node redhat8-x86_64 redhat8-zen2 redhat8-zen2-ampere vaughan MI100 redhat8-x86_64 redhat8-zen2 redhat8-zen2-arcturus login/compute leibniz redhat8-x86_64 redhat8-broadwell redhat8-broadwell-noaccel leibniz visualisation redhat8-x86_64 redhat8-broadwell redhat8-broadwell-P5000 leibniz Pascal redhat8-x86_64 redhat8-broadwell redhat8-broadwell-pascal BioMina node redhat8-x86_64 redhat8-skylake redhat8-skylake-noaccel Leibniz aurora redhat8-x86_64 redhat8-skylake redhat8-skylake-aurora1 Hopper node redhat8-x86_64 redhat8-ivybridge / For Hopper we could drop the highest level as we only have nodes without accelerator and no new nodes with this CPU will ever come in. Advantages: Minimal number of duplicated installations Disadvantages: Very easy to make mistakes about what to install where. No package should be installed with the same full name (name + version + versionsuffix) at multiple levels. No package can have dependencies at a higher level. As it would be dangerous to have both an OpenMPI with and without accelerator support loaded, it means we have to install OpenMPI and all packages that depend on it at level 3, so we need to be very careful here. Everything installed with Intel that does not need a GPU can be installed at level 2 as there is no GPU-specific Intel MPI. With FOSS and its subtoolchains the situation is different. GCCcore and GCC can be installed at level 1 or 2 but as long as we do not know if the EasyBuild community will succeed at building a single MPI module that works for everything, gompi and foss software should be installed at level 3.","title":"Option 1: Maximal common installations"},{"location":"modules_calcua_and_clusterarch/#option-2-only-common-installations-for-software-such-as-matlab-or-maybe-system-toolchain","text":"Now there are two levels: Level 1: Unoptimized generic x86 64-bit CPU Level 2: Optimised software, with 2 options for naming: use the shortest name possible or always use three components in the name: OS, CPU and accelerator. We will not implement the first option for Level 2 as this would only create confusion if we would ever want to mix schemes and switch to the first option for a new version of the software stack. Node L1 (generic) L2 (shortest - not implemented) L2 (3-component) login/compute vaughan redhat8-x86_64 redhat8-zen2 redhat8-zen2-noaccel vaughan Ampere node redhat8-x86_64 redhat8-zen2-ampere redhat8-zen2-ampere vaughan MI100 redhat8-x86_64 redhat8-zen2-arcturus redhat8-zen2-arcturus login/compute leibniz redhat8-x86_64 redhat8-broadwell redhat8-broadwell-noaccel leibniz visualisation redhat8-x86_64 redhat8-broadwell-P5000 redhat8-broadwell-P5000 leibniz Pascal redhat8-x86_64 redhat8-broadwell-pascal redhat8-broadwell-pascal BioMina node redhat8-x86_64 redhat8-skylake redhat8-skylake-noaccel Leibniz aurora redhat8-x86_64 redhat8-skylake-aurora1 redhat8-skylake-aurora1 Hopper node redhat8-x86_64 redhat8-ivybridge redhat8-ivybridge-noaccel Users could in principle still use software from another architecture within the stack by loading the appropriate clusterarch module so we could still be fairly selective about what we provide for the \"special\" nodes with accelerators. However, many of the often recurring dependencies like alternatives for what are often basic OS libraries would have to be installed multiple times. In this case the \"noaccel\" architecture isn't really needed unless we want all names to have three components if they are on level 2 (which may ease a transition to option 1 later on). Advantages Conceptually certainly simpler as there is little doubt about where to install a module Disadvantages Larger volume of the overall software stack as more modules will be duplicated.","title":"Option 2: Only common installations for software such as Matlab or maybe system toolchain"},{"location":"procedures/","text":"Some procedures Procedures in this document: Starting a new software stack Upgrading EasyBuild in an existing software stack Adding a new node type to the CalcUA infrastructure Starting a new software stack Code to check: Table toolchain_map in SitePackage_map_toolchain.lua : Add an entry for the new toolchain. TODO Upgrading EasyBuild in an existing software stack Software stacks on the CalcUA infrastructure may have a long life before they are being superseded with a new one based on newer compilers. Hence it may make sense to upgrade EasyBuild during the life of the software stack. However, upgrading EasyBuild may break things and hence should be done with care and proper testing. TODO Outline safe procedures and practices. Adding a new node type to the CalcuUA infrastructure The following files/routines may need changes: TODO Tables in LMOD/SitePackage_arch_hierarchy.lua Tables in LMOD/SitePackage_system_info.lua to be able to detect the hardware. Check if the code for get_Accelerator_info() in LMOD/SitePackage_system_info.lua is still suitable. The scripts-dev directory contains the script test_sysinfo.lua which is perfect for this purpose. System definition in etc/SystemDefinition.lua (really more to determine which software stacks are available). Upgrading the OS Check the tables for the translation of the OS name as reported by the system to supported names in LMOD/SitePackage_system_info.lua","title":"Procedures"},{"location":"procedures/#some-procedures","text":"Procedures in this document: Starting a new software stack Upgrading EasyBuild in an existing software stack Adding a new node type to the CalcUA infrastructure","title":"Some procedures"},{"location":"procedures/#starting-a-new-software-stack","text":"Code to check: Table toolchain_map in SitePackage_map_toolchain.lua : Add an entry for the new toolchain. TODO","title":"Starting a new software stack"},{"location":"procedures/#upgrading-easybuild-in-an-existing-software-stack","text":"Software stacks on the CalcUA infrastructure may have a long life before they are being superseded with a new one based on newer compilers. Hence it may make sense to upgrade EasyBuild during the life of the software stack. However, upgrading EasyBuild may break things and hence should be done with care and proper testing. TODO Outline safe procedures and practices.","title":"Upgrading EasyBuild in an existing software stack"},{"location":"procedures/#adding-a-new-node-type-to-the-calcuua-infrastructure","text":"The following files/routines may need changes: TODO Tables in LMOD/SitePackage_arch_hierarchy.lua Tables in LMOD/SitePackage_system_info.lua to be able to detect the hardware. Check if the code for get_Accelerator_info() in LMOD/SitePackage_system_info.lua is still suitable. The scripts-dev directory contains the script test_sysinfo.lua which is perfect for this purpose. System definition in etc/SystemDefinition.lua (really more to determine which software stacks are available).","title":"Adding a new node type to the CalcuUA infrastructure"},{"location":"procedures/#upgrading-the-os","text":"Check the tables for the translation of the OS name as reported by the system to supported names in LMOD/SitePackage_system_info.lua","title":"Upgrading the OS"},{"location":"scripts/","text":"Scripts enable_ClusterMod.sh The enable_ClusterMod.sh script prints the shell commands needed to enable the CalcUA stacks in a format suitable to be used with the eval bash shell function. Without arguments it assumes that the software stack is located two levels towards the root of the file system hierarchy from the location of the script. Alternatively it is possible to give the root of the software stack as an argument to the script. Besides setting a number of LMOD_ environment variables to configure Lmod, it also sets the CLUSTERMOD_SOFTWARESTACK environment variable which points to the configuration file (itself a Lua script) of the software stack. Note that we need this variable to find the system definition file which then tells the name of the cluster which is why we cannot use the name of the cluster in the name of the environment variable. check_systemdefinition.sh This script runs a whole set of tests again the system definition file to check for inconsistencies or possible mistakes. Without command line arguments it will check the default file etc/SystemDefinition.lua in the parent directory of the directory containing the check_systemdefinition.sh script, but it is also possible to point the script to a different system definition file on the command line. It is certainly not a guarantee that there are no inconsistencies in the file, though it is an extensive set of tests. check_clusternode.sh The check_clusternode.sh script prints information about the node it is running on and can also be used to check if the node is properly supported by a given system definition. Just as check_systemdefinintion.sh , without command line arguments it will check the default file etc/SystemDefinition.lua in the parent directory of the directory containing the check_clusternode.sh script, but it is also possible to point the script to a different system definition file on the command line. prepare_ClusterMod.sh This script is used to set up or extend the whole structure of the software stack. It should be run whenever new software stack versions are started or new nodes are added to the cluster. It tries to not be destructive to whatever files are already there, but will of course replace them when needed. The script will create the etc/SoftwareStack.lua file in the indicated install directory unless it is run in repair mode. This script is very essential in the whole module system: it helps many other components to find the location of other files. The script has several command line arguments: -h or --help : Print help information and exit. -i or --installroot : Points to the directory where the software stack should be installed. The default value is derived from the location of the script (i.e., two directories towards the root in the hierarchy). -e or --easybuild : The absolute or relative path (with respect to the current directory) of the repository with all custom EasyBuild files. The default is UAntwerpen-easybuild located two directories towards the root of the directory in which the prepare_ClusterMod.sh script is located. -s or --systemdefinition : The system definition file, with an absolute path or path relative to the current directory. The default is etc/SystemDefinition.lua located in the parent directory of the directory where the prepare_ClusterMod.sh script is found. -r or --repair : Runs the script in repair mode. One can still indicate the installation directory and the script will then search in that directory to locate eth etc/SoftwareStack.lua file. However, the -e / --easybuild and -s / --systemdefinition flags should not be used as that data comes from the etc/SoftwareStack.lua file. -d or --debug : Mostly useful during development, prints additional debug information. update_easybuild.sh This script checks if all the necessary versions of EasyBuild are installed. When given without arguments, it tries to locate the SoftwareStack.lua file relative to its own location. However, the root of the installation (which then contains etc/SoftwareStack.lua ) can be given as a command line argument. The versions of EasyBuild that should be installed are derived from the ClusterMod_SystemProperties in the system definition file refered to by the SoftwareStack.lua file. Note that the necessary EasyConfig files should be present, or the script will produce an error message.","title":"Scripts"},{"location":"scripts/#scripts","text":"","title":"Scripts"},{"location":"scripts/#enable_clustermodsh","text":"The enable_ClusterMod.sh script prints the shell commands needed to enable the CalcUA stacks in a format suitable to be used with the eval bash shell function. Without arguments it assumes that the software stack is located two levels towards the root of the file system hierarchy from the location of the script. Alternatively it is possible to give the root of the software stack as an argument to the script. Besides setting a number of LMOD_ environment variables to configure Lmod, it also sets the CLUSTERMOD_SOFTWARESTACK environment variable which points to the configuration file (itself a Lua script) of the software stack. Note that we need this variable to find the system definition file which then tells the name of the cluster which is why we cannot use the name of the cluster in the name of the environment variable.","title":"enable_ClusterMod.sh"},{"location":"scripts/#check_systemdefinitionsh","text":"This script runs a whole set of tests again the system definition file to check for inconsistencies or possible mistakes. Without command line arguments it will check the default file etc/SystemDefinition.lua in the parent directory of the directory containing the check_systemdefinition.sh script, but it is also possible to point the script to a different system definition file on the command line. It is certainly not a guarantee that there are no inconsistencies in the file, though it is an extensive set of tests.","title":"check_systemdefinition.sh"},{"location":"scripts/#check_clusternodesh","text":"The check_clusternode.sh script prints information about the node it is running on and can also be used to check if the node is properly supported by a given system definition. Just as check_systemdefinintion.sh , without command line arguments it will check the default file etc/SystemDefinition.lua in the parent directory of the directory containing the check_clusternode.sh script, but it is also possible to point the script to a different system definition file on the command line.","title":"check_clusternode.sh"},{"location":"scripts/#prepare_clustermodsh","text":"This script is used to set up or extend the whole structure of the software stack. It should be run whenever new software stack versions are started or new nodes are added to the cluster. It tries to not be destructive to whatever files are already there, but will of course replace them when needed. The script will create the etc/SoftwareStack.lua file in the indicated install directory unless it is run in repair mode. This script is very essential in the whole module system: it helps many other components to find the location of other files. The script has several command line arguments: -h or --help : Print help information and exit. -i or --installroot : Points to the directory where the software stack should be installed. The default value is derived from the location of the script (i.e., two directories towards the root in the hierarchy). -e or --easybuild : The absolute or relative path (with respect to the current directory) of the repository with all custom EasyBuild files. The default is UAntwerpen-easybuild located two directories towards the root of the directory in which the prepare_ClusterMod.sh script is located. -s or --systemdefinition : The system definition file, with an absolute path or path relative to the current directory. The default is etc/SystemDefinition.lua located in the parent directory of the directory where the prepare_ClusterMod.sh script is found. -r or --repair : Runs the script in repair mode. One can still indicate the installation directory and the script will then search in that directory to locate eth etc/SoftwareStack.lua file. However, the -e / --easybuild and -s / --systemdefinition flags should not be used as that data comes from the etc/SoftwareStack.lua file. -d or --debug : Mostly useful during development, prints additional debug information.","title":"prepare_ClusterMod.sh"},{"location":"scripts/#update_easybuildsh","text":"This script checks if all the necessary versions of EasyBuild are installed. When given without arguments, it tries to locate the SoftwareStack.lua file relative to its own location. However, the root of the installation (which then contains etc/SoftwareStack.lua ) can be given as a command line argument. The versions of EasyBuild that should be installed are derived from the ClusterMod_SystemProperties in the system definition file refered to by the SoftwareStack.lua file. Note that the necessary EasyConfig files should be present, or the script will produce an error message.","title":"update_easybuild.sh"},{"location":"Development/","text":"Information purely for developers of the module system Test scripts Design considerations","title":"Information purely for developers of the module system"},{"location":"Development/#information-purely-for-developers-of-the-module-system","text":"Test scripts Design considerations","title":"Information purely for developers of the module system"},{"location":"Development/debug_scripts/","text":"Debug scripts in the scripts-dev directory test_helper.lua test_map_toolchain.lua test_arch_hierarchy.lua This script is meant as a test script for SitePackage_arch_hierarchy.lua . It includes its own equivalent of etc/SystemDefinition.lua in the script to ensure predictable results. test_sysinfo.lua Scripts to test concepts used in the implementation test_symlink.lua This is a script to experiment with the lfs.symlinkattributes method.","title":"Test scripts"},{"location":"Development/debug_scripts/#debug-scripts-in-the-scripts-dev-directory","text":"","title":"Debug scripts in the scripts-dev directory"},{"location":"Development/debug_scripts/#test_helperlua","text":"","title":"test_helper.lua"},{"location":"Development/debug_scripts/#test_map_toolchainlua","text":"","title":"test_map_toolchain.lua"},{"location":"Development/debug_scripts/#test_arch_hierarchylua","text":"This script is meant as a test script for SitePackage_arch_hierarchy.lua . It includes its own equivalent of etc/SystemDefinition.lua in the script to ensure predictable results.","title":"test_arch_hierarchy.lua"},{"location":"Development/debug_scripts/#test_sysinfolua","text":"","title":"test_sysinfo.lua"},{"location":"Development/debug_scripts/#scripts-to-test-concepts-used-in-the-implementation","text":"","title":"Scripts to test concepts used in the implementation"},{"location":"Development/debug_scripts/#test_symlinklua","text":"This is a script to experiment with the lfs.symlinkattributes method.","title":"test_symlink.lua"},{"location":"Development/design_considerations/","text":"Design considerations Options EasyBuild-managed software: Option 1 : modules, software and ebrepo at the bottom of the directory hierarchy. So each of these directories than have subdirectories for the various versions of the stack and various hardware/OS architectures. The advantage is that we can keep the length of the path for the software minimal by using abbreviations while we can use more readable names for the modules and ebrepo directories. May also be easier to integrate with the current setup for software that is already installed. Option 2a : First subdirectories per version of the stack, hardware/OS and then modules/software/ebrepo_files Option 2b : hardware/OS, then version of the stack, then modules/software/ebrepo_files This goes contrary to the LMOD hierarchy, though this shouldn\u2019t really matter as it is only important for the infrastructure modules. We should not forget that there is also software outside the stack, generic x86, which is systemwide available. Note that besides the EasyBuild modules, we still need the infrastructure tree (as on LUMI) to get everything to work correctly. The infrastructure module tree follows a strict Lmod hierarchy whereas the tree with EasyBuild-generated modules does not. The infrastructure tree does, e.g., contain the EasyBuild configuration modules and only one subdirectory of it can be in the MODULEPATH at any given time. Which elements determine the architecture string? OS version: certainly CPU architecture: certainly. Propose to use the names that are also used in archspec. Do we want to include GPU architecture? And if so, build upon an existing stack which saves disk space but adds complications in the modules and the risk that dependencies get installed in the wrong place when using eb -r? Do we want to include the interconnect? EESSI doesn\u2019t distinguish between interconnects as they manage to build an MPI that works on Ethernet, InfiniBand and Omni-Path, though it remains to be seen if they can adapt to Slingshot also\u2026 It is only really important if we would have CPUs of the same generation with a different interconnect. It can be added later, though it would mean that sometimes the interconnect is in the architecture string and sometimes it is not (which I think happened in Brussels). Separate repository for everything related to LMOD settings and generic modules to select the version of the software tree and do the EasyBuild settings? A single EasyBuild repository with everything or as now different repositories? Where do we put manually installed software? A common architecture to install tools that only depend on OS (or even not that) such as EasyBuild itself, or simply have multiple copies of these tools also? Gent does the latter to keep it simple, on LUMI we do the former to save a bit on the number of files given the load that lots of small files put on a parallel file system. More detailed layout Option 1 apps \u2514\u2500 antwerpen \u2514\u2500 CalcUA \u251c\u2500 UAntwerpen-modules**: Repository with LMOD configuration and generic modules \u251c\u2500 UAntwerpen-easybuild: EasyBuild setup \u2502 \u2514\u2500 easybuild \u2502 \u251c\u2500 easyconfigs \u2502 \u251c\u2500 easyblocks \u2502 \u251c\u2500 Customisations to naming schemes etc. \u2502 \u2514\u2500 config: Configuration files for some settings not done via environment \u251c\u2500 modules-infrastructure: The structure of modules to select the version etc., in a LMOD hierarchy \u2502 \u251c\u2500 stacks \u2502 \u2502 \u2514\u2500 calcua \u2502 \u2502 \u2514\u2500 2021b.lua: Symbolic link to a generic module! \u2502 \u251c\u2500 arch \u2502 \u2502 \u2514\u2500 calcua \u2502 \u2502 \u2514\u2500 2021b \u2502 \u2502 \u251c\u2500 cluster \u2502 \u2502 \u2502 \u251c\u2500 hopper.lua: Symbolic link to a generic module! \u2502 \u2502 \u2502 \u251c\u2500 leibniz.lua \u2502 \u2502 \u2502 \u251c\u2500 leibniz-skl.lua \u2502 \u2502 \u2502 \u251c\u2500 vaughan.lua \u2502 \u2502 \u2502 \u2514\u2500 generic.lua \u2502 \u2502 \u2514\u2500 arch \u2502 \u2502 \u251c\u2500 redhat8 \u2502 \u2502 \u251c\u2500 redhat8-broadwell \u2502 \u2502 \u2514\u2500 redhat8-broadwell-quadro \u2502 \u2514\u2500 infrastructure \u2502 \u2514\u2500 CalcUA \u2502 \u2514\u2500 2021b \u2502 \u2514\u2500 arch \u2502 \u2514\u2500 redhat8-ivybridge \u2502 \u251c\u2500 EasyBuild-production \u2502 \u251c\u2500 EasyBuild-infrastructure \u2502 \u2514\u2500 EasyBuild-user \u251c\u2500 modules-easybuild \u2502 \u251c\u2500 CalcUA-2021b \u2502 \u2502 \u251c\u2500 redhat8_x86_64 : Directory for potential generic builds if performance does not matter \u2502 \u2502 \u251c\u2500 redhat8-broadwell \u2502 \u2502 \u2514\u2500 redhat8-broadwell-quadro \u2502 \u2514\u2500 system: Modules outside the regular software stacks \u2502 \u251c\u2500 redhat8 : No specific processor versions, e.g., Matlab \u2502 \u2514\u2500 redhat8-ivybridge: Specific processor version, e.g., Gaussian \u2514\u2500 SW \u251c\u2500 CalcUA-2021b \u2502 \u251c\u2500 RH8-x86_64 \u2502 \u251c\u2500 RH8-BRW \u2502 \u2514\u2500 RH8-BRW-NVGP61GL \u251c\u2500 system : Sometimes relatively empty subdirs if EasyBuild only creates a module. \u2502 \u251c\u2500 RH8 \u2502 \u2514\u2500 RH8-IVB \u2514\u2500 MNL : Manually installed software. Do we need an OS level? Probably best \u2514\u2500 RH8-x86_86 Option 2a apps \u2514\u2500 antwerpen \u2514\u2500 CalcUA \u251c\u2500 UAntwerpen-modules: Repository with LMOD configuration and generic modules \u251c\u2500 UAntwerpen-easybuild: EasyBuild setup \u2502 \u2514\u2500 easybuild \u2502 \u251c\u2500 easyconfigs \u2502 \u251c\u2500 easyblocks \u2502 \u251c\u2500 Customisations to naming schemes etc. \u2502 \u2514\u2500 config: Configuration files for some settings not done via environment \u251c\u2500 modules-infrastructure: The structure of modules to select the version etc. \u2502 \u251c\u2500 stacks \u2502 \u2502 \u2514\u2500 calcua \u2502 \u2502 \u2514\u2500 2021b.lua: Symbolic link to a generic module! \u2502 \u251c- arch \u2502 \u2502 \u2514\u2500 calcua \u2502 \u2502 \u2514\u2500 2021b \u2502 \u2502 \u2514\u2500 cluster \u2502 \u2502 \u251c\u2500 hopper.lua: Symbolic link to a generic module! \u2502 \u2502 \u251c\u2500 leibniz.lua \u2502 \u2502 \u251c\u2500 leibniz-skl.lua \u2502 \u2502 \u251c\u2500 vaughan.lua \u2502 \u2502 \u2514\u2500 generic.lua \u2502 \u2514\u2500 infrastructure \u2502 \u2514\u2500 CalcUA \u2502 \u2514\u2500 2021b \u2502 \u2514\u2500 arch \u2502 \u2514\u2500 ivybridge-redhat8 \u2502 \u251c\u2500 EasyBuild-production \u2502 \u251c\u2500 EasyBuild-infrastructure \u2502 \u2514\u2500 EasyBuild-user \u2514\u2500 2021b \u251c\u2500 ivybridge-redhat8* \u2502 \u251c\u2500 software \u2502 \u251c\u2500 modules \u2502 \u2514\u2500 ebrepo_files \u251c\u2500 haswell-redhat8 \u251c\u2500 skylake-redhat8 : Not exactly archspec, then it should be skylake_avx512-redhat8 \u251c\u2500 zen2-redhat8 \u2514\u2500 86_64-redhat8 ????? ( In archspecL x86_64_v2 for IVB, x86_64_v3 for rome/BDW And we may need a directory for automatically generated files (needed on LUMI but may not be needed in Antwerp) Note for the cluster modules: Should we also include the OS in the naming scheme (e.g., to keep software for an old OS available at your own risk)? So far we have seen that MPI software for CentOS 7 breaks on CentOS 8 so it is probably a support nightmare\u2026 It may matter at times when we switch the OS to a new version though I think this can be solved easily even in a generic module by temporary adding modules such as cluster/leibniz-CentOS7 etc.","title":"Design considerations"},{"location":"Development/design_considerations/#design-considerations","text":"","title":"Design considerations"},{"location":"Development/design_considerations/#options","text":"EasyBuild-managed software: Option 1 : modules, software and ebrepo at the bottom of the directory hierarchy. So each of these directories than have subdirectories for the various versions of the stack and various hardware/OS architectures. The advantage is that we can keep the length of the path for the software minimal by using abbreviations while we can use more readable names for the modules and ebrepo directories. May also be easier to integrate with the current setup for software that is already installed. Option 2a : First subdirectories per version of the stack, hardware/OS and then modules/software/ebrepo_files Option 2b : hardware/OS, then version of the stack, then modules/software/ebrepo_files This goes contrary to the LMOD hierarchy, though this shouldn\u2019t really matter as it is only important for the infrastructure modules. We should not forget that there is also software outside the stack, generic x86, which is systemwide available. Note that besides the EasyBuild modules, we still need the infrastructure tree (as on LUMI) to get everything to work correctly. The infrastructure module tree follows a strict Lmod hierarchy whereas the tree with EasyBuild-generated modules does not. The infrastructure tree does, e.g., contain the EasyBuild configuration modules and only one subdirectory of it can be in the MODULEPATH at any given time. Which elements determine the architecture string? OS version: certainly CPU architecture: certainly. Propose to use the names that are also used in archspec. Do we want to include GPU architecture? And if so, build upon an existing stack which saves disk space but adds complications in the modules and the risk that dependencies get installed in the wrong place when using eb -r? Do we want to include the interconnect? EESSI doesn\u2019t distinguish between interconnects as they manage to build an MPI that works on Ethernet, InfiniBand and Omni-Path, though it remains to be seen if they can adapt to Slingshot also\u2026 It is only really important if we would have CPUs of the same generation with a different interconnect. It can be added later, though it would mean that sometimes the interconnect is in the architecture string and sometimes it is not (which I think happened in Brussels). Separate repository for everything related to LMOD settings and generic modules to select the version of the software tree and do the EasyBuild settings? A single EasyBuild repository with everything or as now different repositories? Where do we put manually installed software? A common architecture to install tools that only depend on OS (or even not that) such as EasyBuild itself, or simply have multiple copies of these tools also? Gent does the latter to keep it simple, on LUMI we do the former to save a bit on the number of files given the load that lots of small files put on a parallel file system.","title":"Options"},{"location":"Development/design_considerations/#more-detailed-layout","text":"","title":"More detailed layout"},{"location":"Development/design_considerations/#option-1","text":"apps \u2514\u2500 antwerpen \u2514\u2500 CalcUA \u251c\u2500 UAntwerpen-modules**: Repository with LMOD configuration and generic modules \u251c\u2500 UAntwerpen-easybuild: EasyBuild setup \u2502 \u2514\u2500 easybuild \u2502 \u251c\u2500 easyconfigs \u2502 \u251c\u2500 easyblocks \u2502 \u251c\u2500 Customisations to naming schemes etc. \u2502 \u2514\u2500 config: Configuration files for some settings not done via environment \u251c\u2500 modules-infrastructure: The structure of modules to select the version etc., in a LMOD hierarchy \u2502 \u251c\u2500 stacks \u2502 \u2502 \u2514\u2500 calcua \u2502 \u2502 \u2514\u2500 2021b.lua: Symbolic link to a generic module! \u2502 \u251c\u2500 arch \u2502 \u2502 \u2514\u2500 calcua \u2502 \u2502 \u2514\u2500 2021b \u2502 \u2502 \u251c\u2500 cluster \u2502 \u2502 \u2502 \u251c\u2500 hopper.lua: Symbolic link to a generic module! \u2502 \u2502 \u2502 \u251c\u2500 leibniz.lua \u2502 \u2502 \u2502 \u251c\u2500 leibniz-skl.lua \u2502 \u2502 \u2502 \u251c\u2500 vaughan.lua \u2502 \u2502 \u2502 \u2514\u2500 generic.lua \u2502 \u2502 \u2514\u2500 arch \u2502 \u2502 \u251c\u2500 redhat8 \u2502 \u2502 \u251c\u2500 redhat8-broadwell \u2502 \u2502 \u2514\u2500 redhat8-broadwell-quadro \u2502 \u2514\u2500 infrastructure \u2502 \u2514\u2500 CalcUA \u2502 \u2514\u2500 2021b \u2502 \u2514\u2500 arch \u2502 \u2514\u2500 redhat8-ivybridge \u2502 \u251c\u2500 EasyBuild-production \u2502 \u251c\u2500 EasyBuild-infrastructure \u2502 \u2514\u2500 EasyBuild-user \u251c\u2500 modules-easybuild \u2502 \u251c\u2500 CalcUA-2021b \u2502 \u2502 \u251c\u2500 redhat8_x86_64 : Directory for potential generic builds if performance does not matter \u2502 \u2502 \u251c\u2500 redhat8-broadwell \u2502 \u2502 \u2514\u2500 redhat8-broadwell-quadro \u2502 \u2514\u2500 system: Modules outside the regular software stacks \u2502 \u251c\u2500 redhat8 : No specific processor versions, e.g., Matlab \u2502 \u2514\u2500 redhat8-ivybridge: Specific processor version, e.g., Gaussian \u2514\u2500 SW \u251c\u2500 CalcUA-2021b \u2502 \u251c\u2500 RH8-x86_64 \u2502 \u251c\u2500 RH8-BRW \u2502 \u2514\u2500 RH8-BRW-NVGP61GL \u251c\u2500 system : Sometimes relatively empty subdirs if EasyBuild only creates a module. \u2502 \u251c\u2500 RH8 \u2502 \u2514\u2500 RH8-IVB \u2514\u2500 MNL : Manually installed software. Do we need an OS level? Probably best \u2514\u2500 RH8-x86_86","title":"Option 1"},{"location":"Development/design_considerations/#option-2a","text":"apps \u2514\u2500 antwerpen \u2514\u2500 CalcUA \u251c\u2500 UAntwerpen-modules: Repository with LMOD configuration and generic modules \u251c\u2500 UAntwerpen-easybuild: EasyBuild setup \u2502 \u2514\u2500 easybuild \u2502 \u251c\u2500 easyconfigs \u2502 \u251c\u2500 easyblocks \u2502 \u251c\u2500 Customisations to naming schemes etc. \u2502 \u2514\u2500 config: Configuration files for some settings not done via environment \u251c\u2500 modules-infrastructure: The structure of modules to select the version etc. \u2502 \u251c\u2500 stacks \u2502 \u2502 \u2514\u2500 calcua \u2502 \u2502 \u2514\u2500 2021b.lua: Symbolic link to a generic module! \u2502 \u251c- arch \u2502 \u2502 \u2514\u2500 calcua \u2502 \u2502 \u2514\u2500 2021b \u2502 \u2502 \u2514\u2500 cluster \u2502 \u2502 \u251c\u2500 hopper.lua: Symbolic link to a generic module! \u2502 \u2502 \u251c\u2500 leibniz.lua \u2502 \u2502 \u251c\u2500 leibniz-skl.lua \u2502 \u2502 \u251c\u2500 vaughan.lua \u2502 \u2502 \u2514\u2500 generic.lua \u2502 \u2514\u2500 infrastructure \u2502 \u2514\u2500 CalcUA \u2502 \u2514\u2500 2021b \u2502 \u2514\u2500 arch \u2502 \u2514\u2500 ivybridge-redhat8 \u2502 \u251c\u2500 EasyBuild-production \u2502 \u251c\u2500 EasyBuild-infrastructure \u2502 \u2514\u2500 EasyBuild-user \u2514\u2500 2021b \u251c\u2500 ivybridge-redhat8* \u2502 \u251c\u2500 software \u2502 \u251c\u2500 modules \u2502 \u2514\u2500 ebrepo_files \u251c\u2500 haswell-redhat8 \u251c\u2500 skylake-redhat8 : Not exactly archspec, then it should be skylake_avx512-redhat8 \u251c\u2500 zen2-redhat8 \u2514\u2500 86_64-redhat8 ????? ( In archspecL x86_64_v2 for IVB, x86_64_v3 for rome/BDW And we may need a directory for automatically generated files (needed on LUMI but may not be needed in Antwerp) Note for the cluster modules: Should we also include the OS in the naming scheme (e.g., to keep software for an old OS available at your own risk)? So far we have seen that MPI software for CentOS 7 breaks on CentOS 8 so it is probably a support nightmare\u2026 It may matter at times when we switch the OS to a new version though I think this can be solved easily even in a generic module by temporary adding modules such as cluster/leibniz-CentOS7 etc.","title":"Option 2a"}]}